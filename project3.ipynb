{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2342813f",
   "metadata": {},
   "source": [
    "## Student Name: Shiv Patel\n",
    "## Student Email: shiv.s.patel@ou.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e6ab65",
   "metadata": {},
   "source": [
    "# Project 3: The Smart City Slicker\n",
    "\n",
    "Imagine you are a stakeholder in a rising Smart City and want to know more about themes and concepts about existing smart cities. You also want to know where does your smart city place among others. In this project, you will perform \n",
    "exploratory data analysis, often shortened to EDA, to examine a data from the [2015 Smart City Challenge](https://www.transportation.gov/smartcity) to find facts about the data and communicating those facts through text analysis and visualizations.\n",
    "\n",
    "In order to explore the data and visualize it, some modifications might need to be made to the data along the way. This is often referred to as data preprocessing or cleaning.\n",
    "Though data preprocessing is technically different from EDA, EDA often exposes problems with the data that need to be fixed in order to continue exploring.\n",
    "Because of this tight coupling, you have to clean the data as necessary to help understand the data.\n",
    "\n",
    "In this project, you will apply your knowledge about data cleaning, machine learning, visualizations, and databases to explore smart city applications.\n",
    "\n",
    "**Part 1** of the notebook will explore and clean the data. \\\n",
    "**Part 2** will take the results of the preprocessed data to create models and visualizations.\n",
    "\n",
    "Empty cells are code cells. \n",
    "Cells denoted with [Your Answer Here] are markdown cells.\n",
    "Edit and add as many cells as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8dcba1",
   "metadata": {},
   "source": [
    "Output file for this notebook is shown as a table for display purposes. Note: The city name can be Norman, OK or OK Norman.\n",
    "\n",
    "| city | raw text | clean text | clusterid | topicids | summary | keywords|\n",
    "| -- | -- | -- | -- | -- | -- | -- |\n",
    "|Norman, OK | Test, test , and testing. | test test test | 0 | T1, T2| test | test |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fd47ce",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "The Dataset: 2015 Smart City Challenge Applicants (non-finalist).\n",
    "In this project you will use the applicant's PDFs as a dataset.\n",
    "The dataset is from the U.S Department of Transportation Smart City Challenge.\n",
    "\n",
    "On the website page for the data, you can find some basic information about the challenge. This is an interesting dataset. Think of the questions that you might be able to answer! A few could be:\n",
    "\n",
    "1. Can I identify frequently occurring words that could be removed during data preprocessing?\n",
    "2. Where are the applicants from?\n",
    "3. Are there multiple entries for the same city in different applicantions?\n",
    "4. What are the major themes and concepts from the smart city applicants?\n",
    "\n",
    "Let's load the data!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aace966",
   "metadata": {},
   "source": [
    "## Loading and Handling files\n",
    "\n",
    "Load data from `smartcity/`. \n",
    "\n",
    "To extract the data from the pdf files, use the [pypdf.pdf.PdfFileReader](https://pypdf.readthedocs.io/en/stable/index.html) class.\n",
    "It will allow you to extract pages and pdf files and add them to a data structure (dataframe, list, dictionary, etc).\n",
    "To install the module, use the command `pipenv install pypdf`.\n",
    "You only need to handle PDF files, handling docx is not necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "151c5a06-9d74-41b9-aa8e-90be00607b2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pypdf import PdfReader\n",
    "import os\n",
    "folderPath = 'smartcity/'\n",
    "\n",
    "pdf_files = [f for f in os.listdir(folderPath) if f.endswith('.pdf')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ed6e32",
   "metadata": {},
   "source": [
    "Create a data structure to add the city name and raw text. You can choose to split the city name from the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2e4905f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataFrame = pd.DataFrame(columns=['city', 'raw text'])\n",
    "i = 0\n",
    "for name in pdf_files:\n",
    "    x = name.endswith(\".pdf\")\n",
    "    if x == True:\n",
    "        filePath = 'smartcity/' + name\n",
    "        reader = PdfReader(filePath)\n",
    "        extracted_text = []\n",
    "\n",
    "        # Iterate through each page in the PDF file\n",
    "        for page_num in range(len(reader.pages)):\n",
    "\n",
    "            page = reader.pages[page_num]\n",
    "            # Extract the text from the page\n",
    "            text = page.extract_text()\n",
    "\n",
    "            # Add the extracted text to the list\n",
    "            extracted_text.append(text)\n",
    "        name = name[:-4]\n",
    "        data_dict = {'city': name, 'raw text': extracted_text}\n",
    "    dataFrame = pd.concat([dataFrame, pd.DataFrame(data_dict)], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "02a95983-cd2d-4d1f-992e-1965ec417e42",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               city                                           raw text\n",
      "0     CA Long Beach  Grant Proposal \\nUSDOT SMART CITIES\\nFebruary ...\n",
      "1     CA Long Beach  CONTENTS\\nPART I- VISION NARRATIVE\\nLetters of...\n",
      "2     CA Long Beach  1 THE SMART CITY CHALLENGE | CITY OF LONG BEAC...\n",
      "3     CA Long Beach  2 CITY OF LONG BEACH | THE SMART CITY CHALLENG...\n",
      "4     CA Long Beach  3 THE SMART CITY CHALLENGE | CITY OF LONG BEAC...\n",
      "...             ...                                                ...\n",
      "2061   NY Rochester  Data governance\\n »Establish a Data Hub for co...\n",
      "2062   NY Rochester  consistent set of standards for this challenge...\n",
      "2063   NY Rochester  EVIDENCE OF \\nCAPACITY\\nThe City of Rochester ...\n",
      "2064   NY Rochester  ExecuƟ  ve Commitment:  The City of Rochester ...\n",
      "2065   NY Rochester  ROCHESTER CYCLING ALLIANCE\\nIn partnership wit...\n",
      "\n",
      "[2066 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(dataFrame)\n",
    "dataFrame.to_csv('output.csv', index=False, escapechar='\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "583fa9a6-8e59-4a3d-91e0-a54c57f839b3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 city                                           raw text\n",
      "0        AK Anchorage    CONTENTS \\n1 VISION ...........................\n",
      "1       AL Birmingham  aBirmingham\\nRising\\nBirmingham Rising! Meetin...\n",
      "2       AL Montgomery   \\n \\n U.S. Department of Transportation - “BE...\n",
      "3    AZ Scottsdale AZ    \\n  \\n \\n \\n \\nFederal Agency Name:   U.S. D...\n",
      "4           AZ Tucson  Tucson Smart City Demonstration Proposal\\nPart...\n",
      "..                ...                                                ...\n",
      "64        VA Richmond    \\n \\n \\n   \\n \\n \\n  \\n      Contact Informa...\n",
      "65  VA Virginia Beach    \\n 1.  Project Vision  ........................\n",
      "66         WA Seattle  Beyond Traffic: USDOT Smart City Challenge\\nAp...\n",
      "67         WA Spokane  USDOT Smart City Challenge -  Spokane  \\nPage ...\n",
      "68         WI Madison  Building a Smart Madison  \\nfor Shared Prosper...\n",
      "\n",
      "[69 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# group the rows by filename and combine the text column\n",
    "dataFrame = dataFrame.groupby(\"city\")[\"raw text\"].apply(lambda x: \" \".join(x)).reset_index()\n",
    "print(dataFrame)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5019a8c3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Cleaning Up PDFs\n",
    "\n",
    "One of the more frustrating aspects of PDF is loading the data into a readable format. The first order of business will be to preprocess the data. To start, you can use code provided by Text Analytics with Python, [Chapter 3](https://github.com/dipanjanS/text-analytics-with-python/blob/master/New-Second-Edition/Ch03%20-%20Processing%20and%20Understanding%20Text/Ch03a%20-%20Text%20Wrangling.ipynb): [contractions.py](https://github.com/dipanjanS/text-analytics-with-python/blob/master/New-Second-Edition/Ch05%20-%20Text%20Classification/contractions.py) (Pages 136-137), and [text_normalizer.py](https://github.com/dipanjanS/text-analytics-with-python/blob/master/New-Second-Edition/Ch05%20-%20Text%20Classification/text_normalizer.py) (Pages 155-156). Feel free to download the scripts or add the code directly to the notebook (please note this code is performed on dataframes).\n",
    "\n",
    "In addition to the data cleaning provided by the textbook, you will need to:\n",
    "1. Consider removing terms that may effect clustering and topic modeling. Words to consider are cities, states, common words (smart, city, page, etc.). Keep in mind n-gram combinations are important; this can also be revisited later depending on your model's performance.\n",
    "2. Check the data to remove applicants that text was not processed correctly. Do not remove more than 15 cities from the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "8142e498",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "import unicodedata\n",
    "import re\n",
    "from nltk.corpus import wordnet\n",
    "import collections\n",
    "#from textblob import Word\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "tokenizer = ToktokTokenizer()\n",
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "# nlp_vec = spacy.load('en_vectors_web_lg', parse=True, tag=True, entity=True)\n",
    "\n",
    "CONTRACTION_MAP = {\n",
    "\"ain't\": \"is not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he'll've\": \"he he will have\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"I'd\": \"I would\",\n",
    "\"I'd've\": \"I would have\",\n",
    "\"I'll\": \"I will\",\n",
    "\"I'll've\": \"I will have\",\n",
    "\"I'm\": \"I am\",\n",
    "\"I've\": \"I have\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'd've\": \"i would have\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'll've\": \"i will have\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it'll've\": \"it will have\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she'll've\": \"she will have\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so as\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they'll've\": \"they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what'll've\": \"what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who'll've\": \"who will have\",\n",
    "\"who's\": \"who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you'll've\": \"you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}\n",
    "\n",
    "def strip_html_tags(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    if bool(soup.find()):\n",
    "        [s.extract() for s in soup(['iframe', 'script'])]\n",
    "        stripped_text = soup.get_text()\n",
    "        stripped_text = re.sub(r'[\\r|\\n|\\r\\n]+', '\\n', stripped_text)\n",
    "    else:\n",
    "        stripped_text = text\n",
    "    return stripped_text\n",
    "\n",
    "\n",
    "#def correct_spellings_textblob(tokens):\n",
    "#\treturn [Word(token).correct() for token in tokens]  \n",
    "\n",
    "\n",
    "def simple_porter_stemming(text):\n",
    "    ps = nltk.porter.PorterStemmer()\n",
    "    text = ' '.join([ps.stem(word) for word in text.split()])\n",
    "    return text\n",
    "\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    text = nlp(text)\n",
    "    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_repeated_characters(tokens):\n",
    "    repeat_pattern = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
    "    match_substitution = r'\\1\\2\\3'\n",
    "    def replace(old_word):\n",
    "        if wordnet.synsets(old_word):\n",
    "            return old_word\n",
    "        new_word = repeat_pattern.sub(match_substitution, old_word)\n",
    "        return replace(new_word) if new_word != old_word else new_word\n",
    "            \n",
    "    correct_tokens = [replace(word) for word in tokens]\n",
    "    return correct_tokens\n",
    "\n",
    "\n",
    "def expand_contractions(text, contraction_mapping=CONTRACTION_MAP):\n",
    "    \n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
    "                                      flags=re.IGNORECASE|re.DOTALL)\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match)\\\n",
    "                                if contraction_mapping.get(match)\\\n",
    "                                else contraction_mapping.get(match.lower())                       \n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "        \n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    return expanded_text\n",
    "\n",
    "\n",
    "def remove_accented_chars(text):\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_special_characters(text, remove_digits=False):\n",
    "    pattern = r'[^a-zA-Z0-9\\s]|\\[|\\]' if not remove_digits else r'[^a-zA-Z\\s]|\\[|\\]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_stopwords(text, is_lower_case=False, stopwords=stopword_list):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stopwords]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopwords]\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text\n",
    "\n",
    "def remove_States(text):\n",
    "    pattern = r'Alabama|Alaska|Arizona|Arkansas|California|Colorado|Connecticut|Delaware|Florida|Georgia|Hawaii|Idaho|Illinois|Indiana|Iowa|Kansas|Kentucky|Louisiana|Maine|Maryland|Massachusetts|Michigan|Minnesota|Mississippi|Missouri|Montana|Nebraska|Nevada|New\\sHampshire|New\\sJersey|New\\sMexico|New\\sYork|North\\sCarolina|North\\sDakota|Ohio|Oklahoma|Oregon|Pennsylvania|Rhode\\sIsland|South\\sCarolina|South\\sDakota|Tennessee|Texas|Utah|Vermont|Virginia|Washington|West\\sVirginia|Wisconsin|Wyoming|AL|AK|AZ|AR|CA|CO|CT|DE|FL|GA|HI|ID|IL|IN|IA|KS|KY|LA|ME|MD|MA|MI|MN|MS|MO|MT|NE|NV|NH|NJ|NM|NY|NC|ND|OH|OK|OR|PA|RI|SC|SD|TN|TX|UT|VT|VA|WA|WV|WI|WY'                        \n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text\n",
    "\n",
    "def remove_CommonWords(text):\n",
    "    pattern = r\"U\\.S\\. Department|smartcity|smart|city|page|section|element|concept|appendix|transportation|(\\s(a|b|c|d|e|f|g|h|i|j|k|l|m|n)\\s)\"\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text\n",
    "\n",
    "def remove_Cities(text):\n",
    "    pattern = r'Anchorage|Birmingham|Montgomery|Scottsdale|Tucson|Chula Vista|Fremont|Fresno|Long Beach|Moreno Valley|Oakland|Oceanside|Riverside|Sacramento|San Jose_0|NewHaven|DC_0|Jacksonville|Miami|Orlando|St. Petersburg|Tallahassee|Tampa|Atlanta|Brookhaven|Des Moines|Indianapolis|Louisville|Baton Rogue|New Orleans|Shreveport|Boston|Baltimore|Detroit|Port Huron and Marysville|Minneapolis St Paul|St. Louis|Charlotte|Greensboro|Raleigh|Lincoln|Omaha|Jersey City|Newark|Las Vegas|Reno|Albany Troy Schenectady Saratoga Springs|Buffalo|Mt Vernon Yonkers New Rochelle|Rochester|Akron|Canton|Cleveland|Toledo|Oklahoma City|Tulsa|Providence|Greenville|Chattanooga|Memphis|Nashville|Lubbock|Newport News|Norfolk|Richmond|Virginia Beach|Seattle|Spokane|Madison'                        \n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text\n",
    "\n",
    "def normalize_corpus(corpus, html_stripping=True, contraction_expansion=True,\n",
    "                     accented_char_removal=True, text_lower_case=True, \n",
    "                     text_stemming=False, text_lemmatization=True, \n",
    "                     special_char_removal=True, remove_digits=True,\n",
    "                     stopword_removal=True, removeStates = True, removeCities = True,\n",
    "                     removeCommonWords = True, stopwords=stopword_list):\n",
    "    \n",
    "    normalized_corpus = []\n",
    "    # normalize each document in the corpus\n",
    "    for doc in corpus:\n",
    "\n",
    "        # strip HTML\n",
    "        if html_stripping:\n",
    "            doc = strip_html_tags(doc)\n",
    "\n",
    "        # remove extra newlines\n",
    "        doc = doc.translate(doc.maketrans(\"\\n\\t\\r\", \"   \"))\n",
    "\n",
    "        # remove accented characters\n",
    "        if accented_char_removal:\n",
    "            doc = remove_accented_chars(doc)\n",
    "\n",
    "        # expand contractions    \n",
    "        if contraction_expansion:\n",
    "            doc = expand_contractions(doc)\n",
    "\n",
    "        # lemmatize text\n",
    "        if text_lemmatization:\n",
    "            doc = lemmatize_text(doc)\n",
    "\n",
    "        # stem text\n",
    "        if text_stemming and not text_lemmatization:\n",
    "        \tdoc = simple_porter_stemming(doc)\n",
    "\n",
    "        # remove special characters and\\or digits    \n",
    "        if special_char_removal:\n",
    "            # insert spaces between special characters to isolate them    \n",
    "            special_char_pattern = re.compile(r'([{.(-)!}])')\n",
    "            doc = special_char_pattern.sub(\" \\\\1 \", doc)\n",
    "            doc = remove_special_characters(doc, remove_digits=remove_digits)  \n",
    "\n",
    "        # remove extra whitespace\n",
    "        doc = re.sub(' +', ' ', doc)\n",
    "\n",
    "         # lowercase the text    \n",
    "        if text_lower_case:\n",
    "            doc = doc.lower()\n",
    "\n",
    "         # remove states\n",
    "        if removeStates:\n",
    "            doc = remove_States(doc)\n",
    "        \n",
    "        # remove the common words\n",
    "        if removeCommonWords:\n",
    "            doc = remove_CommonWords(doc)\n",
    "\n",
    "        # remove cities\n",
    "        if removeCities:\n",
    "            doc = remove_Cities(doc)\n",
    "            \n",
    "        # remove stopwords\n",
    "        if stopword_removal:\n",
    "            doc = remove_stopwords(doc, is_lower_case=text_lower_case, stopwords=stopwords)\n",
    "            \n",
    "        # remove extra whitespace\n",
    "        doc = re.sub(' +', ' ', doc)\n",
    "        doc = doc.strip()\n",
    "            \n",
    "        normalized_corpus.append(doc)\n",
    "        \n",
    "    return normalized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "84bf63a9-bf04-44d6-a83f-b454402dba1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "corpus = normalize_corpus(dataFrame['raw text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1473a3e3",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Add the cleaned text to the structure you created.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "a0dc9b31-dd0c-4e22-b9d0-cf84b644048f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataFrame['clean text'] = corpus\n",
    "dataFrame.to_csv('cleaned.csv', index=False, escapechar='\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6525e7a-921b-4376-b32d-d310035aa529",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             filename                                               text\n",
      "0        AK Anchorage  content vision population chaestics site p app...\n",
      "1       AL Birmingham  rise rise meet challenge become americas next ...\n",
      "2       AL Montgomery  u department transportation beyond traffic srt...\n",
      "3    AZ Scottsdale AZ  federal agency name u department transportatio...\n",
      "4           AZ Tucson  smart city demonstration proposal part vision ...\n",
      "..                ...                                                ...\n",
      "64        VA Richmond  contact information contact michael sawyer pho...\n",
      "65  VA Virginia Beach  project vision project demographics project al...\n",
      "66         WA Seattle  beyond traffic uot smart city challenge applic...\n",
      "67         WA Spokane  uot smart city challenge page connected transp...\n",
      "68         WI Madison  build smart shared prosperity application beyo...\n",
      "\n",
      "[69 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "cleaned_text = pd.read_csv('cleaned.csv')\n",
    "print(cleaned_text)\n",
    "cleaned_text = cleaned_text.fillna('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82cc947b",
   "metadata": {},
   "source": [
    "### Clean Up: Discussion\n",
    "Answer the questions below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1ba98d",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Which Smart City applicants did you remove? What issues did you see with the documents?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffebf5a5",
   "metadata": {},
   "source": [
    "The applicants I removed are Moreno Valley, Tallahassee, Reno, Toledo, and Lubbock. PdfReader was not able to read in text from the pdfs of these cities. The rows corresponding to these cities in the dataframe were all black and so there is no use of empty rows which led to the removal of these cities. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1620ed74",
   "metadata": {},
   "source": [
    "#### Explain what additional text processing methods you used and why."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae42fc81",
   "metadata": {},
   "source": [
    "I used 3 functions remove_States(), remove_CommonWords(), remove_Cites() other than the ones provided in the text_normalizer.py to remove words that may effect clustering and topic modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15817355",
   "metadata": {},
   "source": [
    "#### Did you identify any potientally problematic words?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ad6082",
   "metadata": {},
   "source": [
    "I found some problematic words looking at some of the cleaned data which are as follows:\n",
    "cteure\n",
    "nthern\n",
    "knik\n",
    "chlenge\n",
    "automo\n",
    "ci\n",
    "es\n",
    "There are many such words that don't mean anything."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1507fbe",
   "metadata": {},
   "source": [
    "## Experimenting with Clustering Models\n",
    "\n",
    "Now, you'll start to explore models to find the optimal clustering model. In this section, you'll explore [K-means](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html), [Hierarchical](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html), and [DBSCAN](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html#sklearn.cluster.DBSCAN) clustering algorithms.\n",
    "Create these algorithms with k_clusters for K-means and Hierarchical.\n",
    "For each cell in the table provide the [Silhouette score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html#sklearn.metrics.silhouette_score), [Calinski and Harabasz score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.calinski_harabasz_score.html#sklearn.metrics.calinski_harabasz_score), and [Davies-Bouldin score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.davies_bouldin_score.html#sklearn.metrics.davies_bouldin_score).\n",
    "\n",
    "In each cell, create an array to store the values.\n",
    "For example, \n",
    "\n",
    "|Algorithm| k = 9 | k = 18| k = 36 | Optimal k| \n",
    "|--|--|--|--|--|\n",
    "|K-means| [-0.01, 1.08, 0.95]| [-0.01, 1.07, 0.94] | [-0.001, 1.13, 0.90] | [0.03, 1.13, 0.92] |\n",
    "|Hierarchical |[0.02, 1.49, 2.90]| [-0.02, 1.40, 1.87]| [-0.02, 1.41, 1.21] | [0.03, 1.70, 4.24]|\n",
    "|DBSCAN | X | X | X | [-0.13, 1.11, 2.09] |\n",
    "\n",
    "\n",
    "\n",
    "### Optimality \n",
    "You will need to find the optimal k for K-means and Hierarchical algorithms.\n",
    "Find the optimality for k in the range 2 to 50.\n",
    "Provide the code used to generate the optimal k and provide justification for your approach.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da2b033",
   "metadata": {},
   "source": [
    "|Algorithm| k = 9 | k = 18| k = 36 | Optimal k| \n",
    "|--|--|--|--|--|\n",
    "|K-means|--|--|--|--|\n",
    "|Hierarchical |--|--|--|--|\n",
    "|DBSCAN | X | X | X | -- |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "2dc8c7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "import yellowbrick\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b45c5a-2008-4cb1-b7bb-e62f7320b659",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "0d96d23a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Silhouette Score for k=9 is: -0.014452714836265075\n",
      "The Calinski-Harabasz Score for k=9 is: 1.07613455331852\n",
      "The Davies-Bouldin Score for k=9 is: 0.945060522966189\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(ngram_range=(1,3))\n",
    "tfidfdocs = tfidf.fit_transform(raw_documents=dataFrame['clean text'])\n",
    "\n",
    "kmeans = KMeans(n_clusters=9, random_state=42, n_init = 'auto')\n",
    "kmeans.fit_transform(tfidfdocs)\n",
    "\n",
    "s_score = silhouette_score(tfidfdocs, kmeans.labels_)\n",
    "ch_score = calinski_harabasz_score(tfidfdocs.toarray(), kmeans.labels_)\n",
    "db_score = davies_bouldin_score(tfidfdocs.toarray(), kmeans.labels_)\n",
    "\n",
    "print(f\"The Silhouette Score for k=9 is: {s_score}\")\n",
    "print(f\"The Calinski-Harabasz Score for k=9 is: {ch_score}\")\n",
    "print(f\"The Davies-Bouldin Score for k=9 is: {db_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1cbec6f4-3d44-4f36-8aca-d1096df012a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Silhouette Score for k=18 is: -0.010334772486326397\n",
      "The Silhouette Score for k=18 is: 1.0666147540091082\n",
      "The Davies-Bouldin Score for k=18 is: 0.938674560096928\n"
     ]
    }
   ],
   "source": [
    "kmeans = KMeans(n_clusters=18, random_state=42, n_init = 'auto')\n",
    "kmeans.fit_transform(tfidfdocs)\n",
    "\n",
    "s_score = silhouette_score(tfidfdocs, kmeans.labels_)\n",
    "ch_score = calinski_harabasz_score(tfidfdocs.toarray(), kmeans.labels_)\n",
    "db_score = davies_bouldin_score(tfidfdocs.toarray(), kmeans.labels_)\n",
    "\n",
    "print(f\"The Silhouette Score for k=18 is: {s_score}\")\n",
    "print(f\"The Silhouette Score for k=18 is: {ch_score}\")\n",
    "print(f\"The Davies-Bouldin Score for k=18 is: {db_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "36c4139f-4e72-4ab2-b764-70aab8781772",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Silhouette Score for k=36 is: -0.0007395182287630532\n",
      "The Silhouette Score for k=36 is: 1.1273725193727573\n",
      "The Davies-Bouldin Score for k=36 is: 0.8961045487403249\n"
     ]
    }
   ],
   "source": [
    "kmeans = KMeans(n_clusters=36, random_state=42, n_init = 'auto')\n",
    "kmeans.fit_transform(tfidfdocs)\n",
    "\n",
    "s_score = silhouette_score(tfidfdocs, kmeans.labels_)\n",
    "ch_score = calinski_harabasz_score(tfidfdocs.toarray(), kmeans.labels_)\n",
    "db_score = davies_bouldin_score(tfidfdocs.toarray(), kmeans.labels_)\n",
    "\n",
    "print(f\"The Silhouette Score for k=36 is: {s_score}\")\n",
    "print(f\"The Silhouette Score for k=36 is: {ch_score}\")\n",
    "print(f\"The Davies-Bouldin Score for k=36 is: {db_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975e32e1-b89f-4b3d-9aa2-b268b79a9945",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Hierarchical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "42480093-166f-4135-8583-19c7a14a7bcd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Silhouette Score for k=9 is: 0.016669680362206243\n",
      "The Calinski-Harabasz Score for k=9 is: 1.4864568669613392\n",
      "The Davies-Bouldin Score for k=9 is: 2.9049682746839576\n"
     ]
    }
   ],
   "source": [
    "agg_clustering = AgglomerativeClustering(n_clusters=9)\n",
    "agg_clustering.fit(tfidfdocs.toarray())\n",
    "\n",
    "s_score = silhouette_score(tfidfdocs, agg_clustering.labels_)\n",
    "ch_score = calinski_harabasz_score(tfidfdocs.toarray(), agg_clustering.labels_)\n",
    "db_score = davies_bouldin_score(tfidfdocs.toarray(), agg_clustering.labels_)\n",
    "\n",
    "print(f\"The Silhouette Score for k=9 is: {s_score}\")\n",
    "print(f\"The Calinski-Harabasz Score for k=9 is: {ch_score}\")\n",
    "print(f\"The Davies-Bouldin Score for k=9 is: {db_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "894ef92d-21d7-442d-92b3-2f835fdf9a0c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Silhouette Score for k=18 is: -0.017012983124648884\n",
      "The Calinski-Harabasz Score for k=18 is: 1.3897788927282986\n",
      "The Davies-Bouldin Score for k=18 is: 1.8690655635557678\n"
     ]
    }
   ],
   "source": [
    "agg_clustering = AgglomerativeClustering(n_clusters=18)\n",
    "agg_clustering.fit(tfidfdocs.toarray())\n",
    "\n",
    "s_score = silhouette_score(tfidfdocs, agg_clustering.labels_)\n",
    "ch_score = calinski_harabasz_score(tfidfdocs.toarray(), agg_clustering.labels_)\n",
    "db_score = davies_bouldin_score(tfidfdocs.toarray(), agg_clustering.labels_)\n",
    "\n",
    "print(f\"The Silhouette Score for k=18 is: {s_score}\")\n",
    "print(f\"The Calinski-Harabasz Score for k=18 is: {ch_score}\")\n",
    "print(f\"The Davies-Bouldin Score for k=18 is: {db_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "65a9535c-df59-418e-bef7-8965fe068889",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Silhouette Score for k=36 is: -0.019914054556347743\n",
      "The Calinski-Harabasz Score for k=36 is: 1.405724107976957\n",
      "The Davies-Bouldin Score for k=36 is: 1.2137268431055608\n"
     ]
    }
   ],
   "source": [
    "agg_clustering = AgglomerativeClustering(n_clusters=36)\n",
    "agg_clustering.fit(tfidfdocs.toarray())\n",
    "\n",
    "s_score = silhouette_score(tfidfdocs, agg_clustering.labels_)\n",
    "ch_score = calinski_harabasz_score(tfidfdocs.toarray(), agg_clustering.labels_)\n",
    "db_score = davies_bouldin_score(tfidfdocs.toarray(), agg_clustering.labels_)\n",
    "\n",
    "print(f\"The Silhouette Score for k=36 is: {s_score}\")\n",
    "print(f\"The Calinski-Harabasz Score for k=36 is: {ch_score}\")\n",
    "print(f\"The Davies-Bouldin Score for k=36 is: {db_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82514e5f-04b3-49e4-8d3e-60c76dda2536",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f86a100f-f845-4133-9ecf-04ff0eecf907",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Silhouette Score for DBSCAN is: -0.1327183959000816\n",
      "The Calinski-Harabasz Score for DBSCAN is: 1.111580058716288\n",
      "The Davies-Bouldin Score for DBSCAN is: 2.0872961354719326\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "\n",
    "# Perform PCA to reduce the dimensionality of the dataset\n",
    "pca = PCA(n_components=69)\n",
    "tfidf_pca = pca.fit_transform(tfidfdocs.toarray())\n",
    "\n",
    "# Apply DBSCAN to the reduced dataset\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
    "dbscan.fit(tfidf_pca)\n",
    "\n",
    "# Calculate the evaluation metrics\n",
    "s_score = silhouette_score(tfidf_pca, dbscan.labels_)\n",
    "ch_score = calinski_harabasz_score(tfidf_pca, dbscan.labels_)\n",
    "db_score = davies_bouldin_score(tfidf_pca, dbscan.labels_)\n",
    "\n",
    "print(f\"The Silhouette Score for DBSCAN is: {s_score}\")\n",
    "print(f\"The Calinski-Harabasz Score for DBSCAN is: {ch_score}\")\n",
    "print(f\"The Davies-Bouldin Score for DBSCAN is: {db_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca04e7ba-b43d-42ec-8f2b-93940dc1f7f3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Optimal K for KMeans and Hierarchical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "b4d8c04d-fc55-4108-af44-da81572869f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "def find_optimal_k(dataset, k_range):\n",
    "    # Perform PCA to reduce the dimensionality of the dataset\n",
    "    pca = PCA(n_components=min(dataset.shape)-1)\n",
    "    dataset_pca = pca.fit_transform(dataset)\n",
    "    \n",
    "    # Initialize lists to store evaluation scores for K-means and hierarchical clustering\n",
    "    kmeans_scores = []\n",
    "    hier_scores = []\n",
    "    \n",
    "    # Iterate over k values and perform clustering for both K-means and hierarchical clustering\n",
    "    for k in k_range:\n",
    "        # K-means clustering\n",
    "        kmeans = KMeans(n_clusters=k, n_init = 'auto')\n",
    "        kmeans.fit(dataset_pca)\n",
    "        kmeans_silhouette = silhouette_score(dataset_pca, kmeans.labels_)\n",
    "        kmeans_ch = calinski_harabasz_score(dataset_pca, kmeans.labels_)\n",
    "        kmeans_db = davies_bouldin_score(dataset_pca, kmeans.labels_)\n",
    "        kmeans_scores.append([k, kmeans_silhouette, kmeans_ch, kmeans_db])\n",
    "\n",
    "        # Hierarchical clustering\n",
    "        hier = AgglomerativeClustering(n_clusters=k)\n",
    "        hier.fit(dataset_pca)\n",
    "        hier_silhouette = silhouette_score(dataset_pca, hier.labels_)\n",
    "        hier_ch = calinski_harabasz_score(dataset_pca, hier.labels_)\n",
    "        hier_db = davies_bouldin_score(dataset_pca, hier.labels_)\n",
    "        hier_scores.append([k, hier_silhouette, hier_ch, hier_db])\n",
    "\n",
    "    # Convert scores to numpy arrays for easier manipulation\n",
    "    kmeans_scores = np.array(kmeans_scores)\n",
    "    hier_scores = np.array(hier_scores)\n",
    "\n",
    "    # Find optimal k value for K-means clustering based on highest Silhouette score\n",
    "    kmeans_optimal = kmeans_scores[np.argmax(kmeans_scores[:, 1]), 0]\n",
    "\n",
    "    # Find optimal k value for hierarchical clustering based on highest Silhouette score\n",
    "    hier_optimal = hier_scores[np.argmax(hier_scores[:, 1]), 0]\n",
    "\n",
    "    # Return optimal k values along with evaluation scores for both K-means and hierarchical clustering\n",
    "    kmeans_optimal_scores = kmeans_scores[np.argmax(kmeans_scores[:, 1])]\n",
    "    hier_optimal_scores = hier_scores[np.argmax(hier_scores[:, 1])]\n",
    "    return kmeans_optimal, hier_optimal, kmeans_optimal_scores, hier_optimal_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "1877f816-6652-456c-ba10-48763b54f26a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-means optimal k: 3.0\n",
      "Hierarchical optimal k: 4.0\n",
      "K-means scores for optimal k:\n",
      " [3.         0.02916928 1.1276371  0.91685356]\n",
      "Hierarchical scores for optimal k:\n",
      " [4.         0.02603313 1.69738975 4.24188422]\n"
     ]
    }
   ],
   "source": [
    "k_range = range(2, 50)\n",
    "kmeans_optimal, hier_optimal, kmeans_scores, hier_scores = find_optimal_k(tfidfdocs.toarray(), k_range)\n",
    "\n",
    "print(\"K-means optimal k:\", kmeans_optimal)\n",
    "print(\"Hierarchical optimal k:\", hier_optimal)\n",
    "print(\"K-means scores for optimal k:\\n\", kmeans_scores)\n",
    "print(\"Hierarchical scores for optimal k:\\n\", hier_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c20fd69",
   "metadata": {},
   "source": [
    "#### How did you approach finding the optimal k?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753c54da",
   "metadata": {},
   "source": [
    "The function find_optimal_k uses the Silhouette score, Calinski and Harabasz score, and Davies-Bouldin score as evaluation metrics for K-means and hierarchical clustering for a range of k values. It then returns the k value that results in the highest Silhouette score, along with the corresponding evaluation scores for both K-means and hierarchical clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79ec635",
   "metadata": {},
   "source": [
    "#### What algorithm do you believe is the best? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3136b0a4",
   "metadata": {},
   "source": [
    "I would choose hierarchical clustering. It has the highest Calinski and Harabasz score, which indicates good separation between clusters, and a relatively high Silhouette score, which indicates the compactness of clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e45a2a3",
   "metadata": {},
   "source": [
    "### Add Cluster ID to output file\n",
    "In your data structure, add the cluster id for each smart city respectively. Show the to append the clusterid code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "9ad83e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimal k for K-means clustering\n",
    "optimal_k = 3\n",
    "\n",
    "# Fit KMeans model with optimal k\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init='auto')\n",
    "kmeans.fit(tfidfdocs)\n",
    "\n",
    "# Assign cluster labels to each data point\n",
    "cluster_labels = kmeans.labels_\n",
    "\n",
    "# Add cluster id to DataFrame\n",
    "dataFrame['clusterid'] = cluster_labels\n",
    "\n",
    "# Save DataFrame with cluster id to output file\n",
    "dataFrame.to_csv('check.csv', index=False, escapechar='\\\\')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959e7275",
   "metadata": {},
   "source": [
    "### Save Model\n",
    "\n",
    "After finding the best model, it is desirable to have a way to persist the model for future use without having to retrain. Save the model using [model persistance](https://scikit-learn.org/stable/model_persistence.html). This model should be saved in the same directory as this notebook and should be loaded as the model for your `project3.py`.\n",
    "\n",
    "Save the model as `model.pkl`. You do not have to use pickle, but be sure to save the persistance using one of the methods listed in the link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "1000077c-bfeb-429b-b9b9-b1429923c2b0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model.pkl']"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump, load\n",
    "\n",
    "# Select the input variables as X\n",
    "X = dataFrame['clean text']\n",
    "\n",
    "# Select the target variable as Y\n",
    "Y = dataFrame['clusterid']\n",
    "\n",
    "# Create a TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf = vectorizer.fit_transform(X)\n",
    "\n",
    "# Train the KMeans model with optimal k\n",
    "kmeans = KMeans(n_clusters=4, random_state=42, n_init='auto')\n",
    "kmeans.fit(tfidf)\n",
    "\n",
    "# Save the model\n",
    "dump(kmeans, 'model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe5a0c9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Derving Themes and Concepts\n",
    "\n",
    "Perform Topic Modeling on the cleaned data. Provide the top five words for `TOPIC_NUM = Best_k` as defined in the section above. Feel free to reference [Chapter 6](https://github.com/dipanjanS/text-analytics-with-python/tree/master/New-Second-Edition/Ch06%20-%20Text%20Summarization%20and%20Topic%20Models) for more information on Topic Modeling and Summarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "b684bc24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: datum vehicle system project use traffic public transit technology provide\n",
      "Topic 2: e st n r g system use baltimore c transit\n",
      "Topic 3: system vehicle datum use transit technology provide service traffic project\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim import corpora, models\n",
    "\n",
    "# Define the number of topics\n",
    "TOPIC_NUM = 3\n",
    "\n",
    "# Convert the cleaned data to a list of tokens\n",
    "docs = [doc.split() for doc in dataFrame['clean text']]\n",
    "\n",
    "# Create a dictionary of the tokens\n",
    "dictionary = corpora.Dictionary(docs)\n",
    "\n",
    "# Create a document-term matrix\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in docs]\n",
    "\n",
    "# Train the LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(doc_term_matrix, num_topics=TOPIC_NUM, id2word = dictionary, passes=50)\n",
    "\n",
    "# Show the top 10 words for each topic\n",
    "topics = lda_model.show_topics(num_topics=TOPIC_NUM, num_words=10, formatted=False)\n",
    "for i, topic in enumerate(topics):\n",
    "    print(f\"Topic {i+1}: {' '.join([word[0] for word in topic[1]])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915516dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2a3896",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1573fe65",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Extract themes\n",
    "Write a theme for each topic (atleast a sentence each)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9737d8df",
   "metadata": {},
   "source": [
    "[Your Answer]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21cf24c6",
   "metadata": {},
   "source": [
    "[Your Answer]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e191f4",
   "metadata": {},
   "source": [
    "[Your Answer]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ab7df0",
   "metadata": {},
   "source": [
    "### Add Topid ID to output file\n",
    "Add the top two topics for each smart city to the data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e937841",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e568652",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "39f9c240",
   "metadata": {},
   "source": [
    "## Gathering Applicant Summaries and Keywords\n",
    "\n",
    "For each smart city applicant, gather a summary and keywords that are important to that document. You can use gensim to do this. Here are examples of functions that you could use.\n",
    "\n",
    "```python\n",
    "\n",
    "from gensim.summarization import summarize\n",
    "\n",
    "def summary(text, ratio=0.2, word_count=250, split=False):\n",
    "    return summarize(text, ratio= ratio, word_count=word_count, split=split)\n",
    "    \n",
    "from gensim.summarization import keywords\n",
    "\n",
    "def keys(text, ratio=0.01):\n",
    "    return keywords(text, ratio=ratio)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "a86cbe26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.summarization.summarizer import summarize\n",
    "from gensim.summarization.keywords import keywords\n",
    "\n",
    "def summary(text, ratio=0.2, word_count=250, split=False):\n",
    "    return summarize(text, ratio= ratio, word_count=word_count, split=split)\n",
    "\n",
    "def keys(text, ratio=0.01):\n",
    "    return keywords(text, ratio=ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "149e91a4-ae42-4ede-bb1c-960ee76f05dc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'Graph' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[237], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dataFrame[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msummary\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdataFrame\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mraw text\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msummary\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mword_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m dataFrame[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeywords\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m dataFrame[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraw text\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: keys(\u001b[38;5;28mstr\u001b[39m(x), ratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m))\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/shivpatel-wnaOTLAh/lib/python3.10/site-packages/pandas/core/series.py:4626\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4516\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4517\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4518\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4521\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4522\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4523\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4524\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4525\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4624\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4625\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/shivpatel-wnaOTLAh/lib/python3.10/site-packages/pandas/core/apply.py:1025\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;66;03m# self.f is Callable\u001b[39;00m\n\u001b[0;32m-> 1025\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/shivpatel-wnaOTLAh/lib/python3.10/site-packages/pandas/core/apply.py:1076\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1074\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1075\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[0;32m-> 1076\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1077\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1078\u001b[0m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1079\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1080\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1082\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1083\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1085\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/shivpatel-wnaOTLAh/lib/python3.10/site-packages/pandas/_libs/lib.pyx:2834\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[237], line 1\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dataFrame[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msummary\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m dataFrame[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraw text\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43msummary\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mword_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      2\u001b[0m dataFrame[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeywords\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m dataFrame[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraw text\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: keys(\u001b[38;5;28mstr\u001b[39m(x), ratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m))\n",
      "Cell \u001b[0;32mIn[104], line 5\u001b[0m, in \u001b[0;36msummary\u001b[0;34m(text, ratio, word_count, split)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msummary\u001b[39m(text, ratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, word_count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m250\u001b[39m, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m----> 5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msummarize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mratio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mword_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mword_count\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/shivpatel-wnaOTLAh/lib/python3.10/site-packages/gensim/summarization/summarizer.py:430\u001b[0m, in \u001b[0;36msummarize\u001b[0;34m(text, ratio, word_count, split)\u001b[0m\n\u001b[1;32m    426\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput text is expected to have at least \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m sentences.\u001b[39m\u001b[38;5;124m\"\u001b[39m, INPUT_MIN_LENGTH)\n\u001b[1;32m    428\u001b[0m corpus \u001b[38;5;241m=\u001b[39m _build_corpus(sentences)\n\u001b[0;32m--> 430\u001b[0m most_important_docs \u001b[38;5;241m=\u001b[39m \u001b[43msummarize_corpus\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mratio\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mword_count\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;66;03m# If couldn't get important docs, the algorithm ends.\u001b[39;00m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m most_important_docs:\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/shivpatel-wnaOTLAh/lib/python3.10/site-packages/gensim/summarization/summarizer.py:369\u001b[0m, in \u001b[0;36msummarize_corpus\u001b[0;34m(corpus, ratio)\u001b[0m\n\u001b[1;32m    366\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease add more sentences to the text. The number of reachable nodes is below 3\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    367\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m []\n\u001b[0;32m--> 369\u001b[0m pagerank_scores \u001b[38;5;241m=\u001b[39m \u001b[43m_pagerank\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    371\u001b[0m hashable_corpus\u001b[38;5;241m.\u001b[39msort(key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m doc: pagerank_scores\u001b[38;5;241m.\u001b[39mget(doc, \u001b[38;5;241m0\u001b[39m), reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mlist\u001b[39m(doc) \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m hashable_corpus[:\u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(corpus) \u001b[38;5;241m*\u001b[39m ratio)]]\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/shivpatel-wnaOTLAh/lib/python3.10/site-packages/gensim/summarization/pagerank_weighted.py:66\u001b[0m, in \u001b[0;36mpagerank_weighted\u001b[0;34m(graph, damping)\u001b[0m\n\u001b[1;32m     62\u001b[0m     pagerank_matrix = damping * adjacency_matrix.todense() + (1 - damping) * probability_matrix\n\u001b[1;32m     64\u001b[0m     vec = principal_eigenvector(pagerank_matrix.T)\n\u001b[0;32m---> 66\u001b[0m     # Because pagerank_matrix is positive, vec is always real (i.e. not complex)\n\u001b[1;32m     67\u001b[0m     return process_results(graph, vec.real)\n\u001b[1;32m     70\u001b[0m def build_adjacency_matrix(graph):\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'Graph' has no len()"
     ]
    }
   ],
   "source": [
    "dataFrame['summary'] = dataFrame['raw text'].apply(lambda x: summary(str(x), ratio=0.2, word_count=50))\n",
    "dataFrame['keywords'] = dataFrame['raw text'].apply(lambda x: keys(str(x), ratio=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "b6b54596-f30d-4b8d-966b-0f9f81ebe2ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     content vision population chaestics site p app...\n",
      "1     rise rise meet challenge become americas next ...\n",
      "2     u department transportation beyond traffic srt...\n",
      "3     federal agency name u department transportatio...\n",
      "4     smart city demonstration proposal part vision ...\n",
      "                            ...                        \n",
      "64    contact information contact michael sawyer pho...\n",
      "65    project vision project demographics project al...\n",
      "66    beyond traffic uot smart city challenge applic...\n",
      "67    uot smart city challenge page connected transp...\n",
      "68    build smart shared prosperity application beyo...\n",
      "Name: clean text, Length: 69, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(dataFrame['clean text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "76430d63-c6ca-4cc6-a451-ab2ac9c13e5c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 city                                           raw text   \n",
      "0        AK Anchorage    CONTENTS \\n1 VISION ...........................  \\\n",
      "1       AL Birmingham  aBirmingham\\nRising\\nBirmingham Rising! Meetin...   \n",
      "2       AL Montgomery   \\n \\n U.S. Department of Transportation - “BE...   \n",
      "3    AZ Scottsdale AZ    \\n  \\n \\n \\n \\nFederal Agency Name:   U.S. D...   \n",
      "4           AZ Tucson  Tucson Smart City Demonstration Proposal\\nPart...   \n",
      "..                ...                                                ...   \n",
      "64        VA Richmond    \\n \\n \\n   \\n \\n \\n  \\n      Contact Informa...   \n",
      "65  VA Virginia Beach    \\n 1.  Project Vision  ........................   \n",
      "66         WA Seattle  Beyond Traffic: USDOT Smart City Challenge\\nAp...   \n",
      "67         WA Spokane  USDOT Smart City Challenge -  Spokane  \\nPage ...   \n",
      "68         WI Madison  Building a Smart Madison  \\nfor Shared Prosper...   \n",
      "\n",
      "                                           clean text  clusterid  \n",
      "0   content vision population chaestics site p app...          1  \n",
      "1   rise rise meet challenge become americas next ...          1  \n",
      "2   u department transportation beyond traffic srt...          1  \n",
      "3   federal agency name u department transportatio...          1  \n",
      "4   smart city demonstration proposal part vision ...          1  \n",
      "..                                                ...        ...  \n",
      "64  contact information contact michael sawyer pho...          1  \n",
      "65  project vision project demographics project al...          1  \n",
      "66  beyond traffic uot smart city challenge applic...          1  \n",
      "67  uot smart city challenge page connected transp...          1  \n",
      "68  build smart shared prosperity application beyo...          1  \n",
      "\n",
      "[69 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "print(dataFrame)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d27ce37",
   "metadata": {},
   "source": [
    "### Add Summaries and Keywords\n",
    "Add summary and keywords to output file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09357ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4f9cb2c4",
   "metadata": {},
   "source": [
    "## Write output data\n",
    "\n",
    "The output data should be written as a TSV file.\n",
    "You can use `to_csv` method from Pandas for this if you are using a DataFrame.\n",
    "\n",
    "`Syntax: df.to_csv('file.tsv', sep = '')` \\\n",
    "`df.to_csv('smartcity_eda.tsv', sep='\\t')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58827464",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7a18b8ff",
   "metadata": {},
   "source": [
    "# Moving Forward\n",
    "Now that you have explored the dataset, take the important features and functions to create your `project3.py`.\n",
    "Please refer to the project spec for more guidance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6675ba",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
