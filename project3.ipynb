{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2342813f",
   "metadata": {},
   "source": [
    "## Student Name: Shiv Patel\n",
    "## Student Email: shiv.s.patel@ou.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e6ab65",
   "metadata": {},
   "source": [
    "# Project 3: The Smart City Slicker\n",
    "\n",
    "Imagine you are a stakeholder in a rising Smart City and want to know more about themes and concepts about existing smart cities. You also want to know where does your smart city place among others. In this project, you will perform \n",
    "exploratory data analysis, often shortened to EDA, to examine a data from the [2015 Smart City Challenge](https://www.transportation.gov/smartcity) to find facts about the data and communicating those facts through text analysis and visualizations.\n",
    "\n",
    "In order to explore the data and visualize it, some modifications might need to be made to the data along the way. This is often referred to as data preprocessing or cleaning.\n",
    "Though data preprocessing is technically different from EDA, EDA often exposes problems with the data that need to be fixed in order to continue exploring.\n",
    "Because of this tight coupling, you have to clean the data as necessary to help understand the data.\n",
    "\n",
    "In this project, you will apply your knowledge about data cleaning, machine learning, visualizations, and databases to explore smart city applications.\n",
    "\n",
    "**Part 1** of the notebook will explore and clean the data. \\\n",
    "**Part 2** will take the results of the preprocessed data to create models and visualizations.\n",
    "\n",
    "Empty cells are code cells. \n",
    "Cells denoted with [Your Answer Here] are markdown cells.\n",
    "Edit and add as many cells as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8dcba1",
   "metadata": {},
   "source": [
    "Output file for this notebook is shown as a table for display purposes. Note: The city name can be Norman, OK or OK Norman.\n",
    "\n",
    "| city | raw text | clean text | clusterid | topicids | summary | keywords|\n",
    "| -- | -- | -- | -- | -- | -- | -- |\n",
    "|Norman, OK | Test, test , and testing. | test test test | 0 | T1, T2| test | test |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fd47ce",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "The Dataset: 2015 Smart City Challenge Applicants (non-finalist).\n",
    "In this project you will use the applicant's PDFs as a dataset.\n",
    "The dataset is from the U.S Department of Transportation Smart City Challenge.\n",
    "\n",
    "On the website page for the data, you can find some basic information about the challenge. This is an interesting dataset. Think of the questions that you might be able to answer! A few could be:\n",
    "\n",
    "1. Can I identify frequently occurring words that could be removed during data preprocessing?\n",
    "2. Where are the applicants from?\n",
    "3. Are there multiple entries for the same city in different applicantions?\n",
    "4. What are the major themes and concepts from the smart city applicants?\n",
    "\n",
    "Let's load the data!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aace966",
   "metadata": {},
   "source": [
    "## Loading and Handling files\n",
    "\n",
    "Load data from `smartcity/`. \n",
    "\n",
    "To extract the data from the pdf files, use the [pypdf.pdf.PdfFileReader](https://pypdf.readthedocs.io/en/stable/index.html) class.\n",
    "It will allow you to extract pages and pdf files and add them to a data structure (dataframe, list, dictionary, etc).\n",
    "To install the module, use the command `pipenv install pypdf`.\n",
    "You only need to handle PDF files, handling docx is not necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "151c5a06-9d74-41b9-aa8e-90be00607b2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pypdf import PdfReader\n",
    "import os\n",
    "folderPath = 'smartcity/'\n",
    "\n",
    "pdf_files = [f for f in os.listdir(folderPath) if f.endswith('.pdf')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ed6e32",
   "metadata": {},
   "source": [
    "Create a data structure to add the city name and raw text. You can choose to split the city name from the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e4905f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataFrame = pd.DataFrame(columns=['filename', 'text'])\n",
    "i = 0\n",
    "for name in pdf_files:\n",
    "    x = name.endswith(\".pdf\")\n",
    "    if x == True:\n",
    "        filePath = 'smartcity/' + name\n",
    "        reader = PdfReader(filePath)\n",
    "        extracted_text = []\n",
    "\n",
    "        # Iterate through each page in the PDF file\n",
    "        for page_num in range(len(reader.pages)):\n",
    "\n",
    "            page = reader.pages[page_num]\n",
    "            # Extract the text from the page\n",
    "            text = page.extract_text()\n",
    "\n",
    "            # Add the extracted text to the list\n",
    "            extracted_text.append(text)\n",
    "        name = name[:-4]\n",
    "        data_dict = {'filename': name, 'text': extracted_text}\n",
    "    dataFrame = pd.concat([dataFrame, pd.DataFrame(data_dict)], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02a95983-cd2d-4d1f-992e-1965ec417e42",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           filename                                               text\n",
      "0     CA Long Beach  Grant Proposal \\nUSDOT SMART CITIES\\nFebruary ...\n",
      "1     CA Long Beach  CONTENTS\\nPART I- VISION NARRATIVE\\nLetters of...\n",
      "2     CA Long Beach  1 THE SMART CITY CHALLENGE | CITY OF LONG BEAC...\n",
      "3     CA Long Beach  2 CITY OF LONG BEACH | THE SMART CITY CHALLENG...\n",
      "4     CA Long Beach  3 THE SMART CITY CHALLENGE | CITY OF LONG BEAC...\n",
      "...             ...                                                ...\n",
      "2061   NY Rochester  Data governance\\n »Establish a Data Hub for co...\n",
      "2062   NY Rochester  consistent set of standards for this challenge...\n",
      "2063   NY Rochester  EVIDENCE OF \\nCAPACITY\\nThe City of Rochester ...\n",
      "2064   NY Rochester  ExecuƟ  ve Commitment:  The City of Rochester ...\n",
      "2065   NY Rochester  ROCHESTER CYCLING ALLIANCE\\nIn partnership wit...\n",
      "\n",
      "[2066 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(dataFrame)\n",
    "dataFrame.to_csv('output.csv', index=False, escapechar='\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "583fa9a6-8e59-4a3d-91e0-a54c57f839b3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             filename                                               text\n",
      "0        AK Anchorage    CONTENTS \\n1 VISION ...........................\n",
      "1       AL Birmingham  aBirmingham\\nRising\\nBirmingham Rising! Meetin...\n",
      "2       AL Montgomery   \\n \\n U.S. Department of Transportation - “BE...\n",
      "3    AZ Scottsdale AZ    \\n  \\n \\n \\n \\nFederal Agency Name:   U.S. D...\n",
      "4           AZ Tucson  Tucson Smart City Demonstration Proposal\\nPart...\n",
      "..                ...                                                ...\n",
      "64        VA Richmond    \\n \\n \\n   \\n \\n \\n  \\n      Contact Informa...\n",
      "65  VA Virginia Beach    \\n 1.  Project Vision  ........................\n",
      "66         WA Seattle  Beyond Traffic: USDOT Smart City Challenge\\nAp...\n",
      "67         WA Spokane  USDOT Smart City Challenge -  Spokane  \\nPage ...\n",
      "68         WI Madison  Building a Smart Madison  \\nfor Shared Prosper...\n",
      "\n",
      "[69 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# group the rows by filename and combine the text column\n",
    "df_combined = dataFrame.groupby(\"filename\")[\"text\"].apply(lambda x: \" \".join(x)).reset_index()\n",
    "print(df_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906148e9-58d2-4c4e-9031-008008f4dae4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5019a8c3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Cleaning Up PDFs\n",
    "\n",
    "One of the more frustrating aspects of PDF is loading the data into a readable format. The first order of business will be to preprocess the data. To start, you can use code provided by Text Analytics with Python, [Chapter 3](https://github.com/dipanjanS/text-analytics-with-python/blob/master/New-Second-Edition/Ch03%20-%20Processing%20and%20Understanding%20Text/Ch03a%20-%20Text%20Wrangling.ipynb): [contractions.py](https://github.com/dipanjanS/text-analytics-with-python/blob/master/New-Second-Edition/Ch05%20-%20Text%20Classification/contractions.py) (Pages 136-137), and [text_normalizer.py](https://github.com/dipanjanS/text-analytics-with-python/blob/master/New-Second-Edition/Ch05%20-%20Text%20Classification/text_normalizer.py) (Pages 155-156). Feel free to download the scripts or add the code directly to the notebook (please note this code is performed on dataframes).\n",
    "\n",
    "In addition to the data cleaning provided by the textbook, you will need to:\n",
    "1. Consider removing terms that may effect clustering and topic modeling. Words to consider are cities, states, common words (smart, city, page, etc.). Keep in mind n-gram combinations are important; this can also be revisited later depending on your model's performance.\n",
    "2. Check the data to remove applicants that text was not processed correctly. Do not remove more than 15 cities from the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8142e498",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "import unicodedata\n",
    "import re\n",
    "from nltk.corpus import wordnet\n",
    "import collections\n",
    "#from textblob import Word\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "tokenizer = ToktokTokenizer()\n",
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "# nlp_vec = spacy.load('en_vectors_web_lg', parse=True, tag=True, entity=True)\n",
    "\n",
    "CONTRACTION_MAP = {\n",
    "\"ain't\": \"is not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he'll've\": \"he he will have\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"I'd\": \"I would\",\n",
    "\"I'd've\": \"I would have\",\n",
    "\"I'll\": \"I will\",\n",
    "\"I'll've\": \"I will have\",\n",
    "\"I'm\": \"I am\",\n",
    "\"I've\": \"I have\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'd've\": \"i would have\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'll've\": \"i will have\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it'll've\": \"it will have\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she'll've\": \"she will have\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so as\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they'll've\": \"they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what'll've\": \"what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who'll've\": \"who will have\",\n",
    "\"who's\": \"who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you'll've\": \"you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}\n",
    "\n",
    "def strip_html_tags(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    if bool(soup.find()):\n",
    "        [s.extract() for s in soup(['iframe', 'script'])]\n",
    "        stripped_text = soup.get_text()\n",
    "        stripped_text = re.sub(r'[\\r|\\n|\\r\\n]+', '\\n', stripped_text)\n",
    "    else:\n",
    "        stripped_text = text\n",
    "    return stripped_text\n",
    "\n",
    "\n",
    "#def correct_spellings_textblob(tokens):\n",
    "#\treturn [Word(token).correct() for token in tokens]  \n",
    "\n",
    "\n",
    "def simple_porter_stemming(text):\n",
    "    ps = nltk.porter.PorterStemmer()\n",
    "    text = ' '.join([ps.stem(word) for word in text.split()])\n",
    "    return text\n",
    "\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    text = nlp(text)\n",
    "    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_repeated_characters(tokens):\n",
    "    repeat_pattern = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
    "    match_substitution = r'\\1\\2\\3'\n",
    "    def replace(old_word):\n",
    "        if wordnet.synsets(old_word):\n",
    "            return old_word\n",
    "        new_word = repeat_pattern.sub(match_substitution, old_word)\n",
    "        return replace(new_word) if new_word != old_word else new_word\n",
    "            \n",
    "    correct_tokens = [replace(word) for word in tokens]\n",
    "    return correct_tokens\n",
    "\n",
    "\n",
    "def expand_contractions(text, contraction_mapping=CONTRACTION_MAP):\n",
    "    \n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
    "                                      flags=re.IGNORECASE|re.DOTALL)\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match)\\\n",
    "                                if contraction_mapping.get(match)\\\n",
    "                                else contraction_mapping.get(match.lower())                       \n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "        \n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    return expanded_text\n",
    "\n",
    "\n",
    "def remove_accented_chars(text):\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_special_characters(text, remove_digits=False):\n",
    "    pattern = r'[^a-zA-Z0-9\\s]|\\[|\\]' if not remove_digits else r'[^a-zA-Z\\s]|\\[|\\]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_stopwords(text, is_lower_case=False, stopwords=stopword_list):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stopwords]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopwords]\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text\n",
    "\n",
    "def remove_States(text):\n",
    "    pattern = r'Alabama|Alaska|Arizona|Arkansas|California|Colorado|Connecticut|Delaware|Florida|Georgia|Hawaii|Idaho|Illinois|Indiana|Iowa|Kansas|Kentucky|Louisiana|Maine|Maryland|Massachusetts|Michigan|Minnesota|Mississippi|Missouri|Montana|Nebraska|Nevada|New\\sHampshire|New\\sJersey|New\\sMexico|New\\sYork|North\\sCarolina|North\\sDakota|Ohio|Oklahoma|Oregon|Pennsylvania|Rhode\\sIsland|South\\sCarolina|South\\sDakota|Tennessee|Texas|Utah|Vermont|Virginia|Washington|West\\sVirginia|Wisconsin|Wyoming|AL|AK|AZ|AR|CA|CO|CT|DE|FL|GA|HI|ID|IL|IN|IA|KS|KY|LA|ME|MD|MA|MI|MN|MS|MO|MT|NE|NV|NH|NJ|NM|NY|NC|ND|OH|OK|OR|PA|RI|SC|SD|TN|TX|UT|VT|VA|WA|WV|WI|WY'                        \n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text\n",
    "\n",
    "def remove_CommonWords(text):\n",
    "    pattern = r\"U\\.S\\. Department|smartcity|smart|city|page|section|element|concept|appendix\"\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text\n",
    "\n",
    "def remove_Cities(text):\n",
    "    pattern = r'Anchorage|Birmingham|Montgomery|Scottsdale|Tucson|Chula Vista|Fremont|Fresno|Long Beach|Moreno Valley|Oakland|Oceanside|Riverside|Sacramento|San Jose_0|NewHaven|DC_0|Jacksonville|Miami|Orlando|St. Petersburg|Tallahassee|Tampa|Atlanta|Brookhaven|Des Moines|Indianapolis|Louisville|Baton Rogue|New Orleans|Shreveport|Boston|Baltimore|Detroit|Port Huron and Marysville|Minneapolis St Paul|St. Louis|Charlotte|Greensboro|Raleigh|Lincoln|Omaha|Jersey City|Newark|Las Vegas|Reno|Albany Troy Schenectady Saratoga Springs|Buffalo|Mt Vernon Yonkers New Rochelle|Rochester|Akron|Canton|Cleveland|Toledo|Oklahoma City|Tulsa|Providence|Greenville|Chattanooga|Memphis|Nashville|Lubbock|Newport News|Norfolk|Richmond|Virginia Beach|Seattle|Spokane|Madison'                        \n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text\n",
    "\n",
    "def normalize_corpus(corpus, html_stripping=True, contraction_expansion=True,\n",
    "                     accented_char_removal=True, text_lower_case=True, \n",
    "                     text_stemming=False, text_lemmatization=True, \n",
    "                     special_char_removal=True, remove_digits=True,\n",
    "                     stopword_removal=True, removeStates = True, removeCities = True,\n",
    "                     removeCommonWords = True, stopwords=stopword_list):\n",
    "    \n",
    "    normalized_corpus = []\n",
    "    # normalize each document in the corpus\n",
    "    for doc in corpus:\n",
    "\n",
    "        # strip HTML\n",
    "        if html_stripping:\n",
    "            doc = strip_html_tags(doc)\n",
    "\n",
    "        # remove extra newlines\n",
    "        doc = doc.translate(doc.maketrans(\"\\n\\t\\r\", \"   \"))\n",
    "\n",
    "        # remove accented characters\n",
    "        if accented_char_removal:\n",
    "            doc = remove_accented_chars(doc)\n",
    "\n",
    "        # expand contractions    \n",
    "        if contraction_expansion:\n",
    "            doc = expand_contractions(doc)\n",
    "\n",
    "        # lemmatize text\n",
    "        if text_lemmatization:\n",
    "            doc = lemmatize_text(doc)\n",
    "\n",
    "        # stem text\n",
    "        if text_stemming and not text_lemmatization:\n",
    "        \tdoc = simple_porter_stemming(doc)\n",
    "\n",
    "        # remove special characters and\\or digits    \n",
    "        if special_char_removal:\n",
    "            # insert spaces between special characters to isolate them    \n",
    "            special_char_pattern = re.compile(r'([{.(-)!}])')\n",
    "            doc = special_char_pattern.sub(\" \\\\1 \", doc)\n",
    "            doc = remove_special_characters(doc, remove_digits=remove_digits)  \n",
    "\n",
    "        # remove extra whitespace\n",
    "        doc = re.sub(' +', ' ', doc)\n",
    "        \n",
    "        # remove states\n",
    "        if removeStates:\n",
    "            doc = remove_States(doc)\n",
    "        \n",
    "        # remove the common words\n",
    "        if removeCommonWords:\n",
    "            doc = remove_CommonWords(doc)\n",
    "\n",
    "        # remove cities\n",
    "        if removeCities:\n",
    "            doc = remove_Cities(doc)\n",
    "\n",
    "         # lowercase the text    \n",
    "        if text_lower_case:\n",
    "            doc = doc.lower()\n",
    "\n",
    "        # remove stopwords\n",
    "        if stopword_removal:\n",
    "            doc = remove_stopwords(doc, is_lower_case=text_lower_case, stopwords=stopwords)\n",
    "            \n",
    "        # remove extra whitespace\n",
    "        doc = re.sub(' +', ' ', doc)\n",
    "        doc = doc.strip()\n",
    "            \n",
    "        normalized_corpus.append(doc)\n",
    "        \n",
    "    return normalized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84bf63a9-bf04-44d6-a83f-b454402dba1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "corpus = normalize_corpus(df_combined['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1473a3e3",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Add the cleaned text to the structure you created.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0dc9b31-dd0c-4e22-b9d0-cf84b644048f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_combined' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m cleaned_text \u001b[38;5;241m=\u001b[39m \u001b[43mdf_combined\u001b[49m\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m      2\u001b[0m cleaned_text[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m corpus\n\u001b[1;32m      3\u001b[0m cleaned_text\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcleaned.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, escapechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_combined' is not defined"
     ]
    }
   ],
   "source": [
    "cleaned_text = df_combined.copy()\n",
    "cleaned_text['text'] = corpus\n",
    "cleaned_text.to_csv('cleaned.csv', index=False, escapechar='\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6525e7a-921b-4376-b32d-d310035aa529",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             filename                                               text\n",
      "0        AK Anchorage  content vision population chaestics site p app...\n",
      "1       AL Birmingham  rise rise meet challenge become americas next ...\n",
      "2       AL Montgomery  u department transportation beyond traffic srt...\n",
      "3    AZ Scottsdale AZ  federal agency name u department transportatio...\n",
      "4           AZ Tucson  smart city demonstration proposal part vision ...\n",
      "..                ...                                                ...\n",
      "64        VA Richmond  contact information contact michael sawyer pho...\n",
      "65  VA Virginia Beach  project vision project demographics project al...\n",
      "66         WA Seattle  beyond traffic uot smart city challenge applic...\n",
      "67         WA Spokane  uot smart city challenge page connected transp...\n",
      "68         WI Madison  build smart shared prosperity application beyo...\n",
      "\n",
      "[69 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "cleaned_text = pd.read_csv('cleaned.csv')\n",
    "print(cleaned_text)\n",
    "cleaned_text = cleaned_text.fillna('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82cc947b",
   "metadata": {},
   "source": [
    "### Clean Up: Discussion\n",
    "Answer the questions below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1ba98d",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Which Smart City applicants did you remove? What issues did you see with the documents?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffebf5a5",
   "metadata": {},
   "source": [
    "The applicants I removed are Moreno Valley, Tallahassee, Reno, Toledo, and Lubbock. PdfReader was not able to read in text from the pdfs of these cities. The rows corresponding to these cities in the dataframe were all black and so there is no use of empty rows which led to the removal of these cities. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1620ed74",
   "metadata": {},
   "source": [
    "#### Explain what additional text processing methods you used and why."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae42fc81",
   "metadata": {},
   "source": [
    "I used 3 functions remove_States(), remove_CommonWords(), remove_Cites() other than the ones provided in the text_normalizer.py to remove words that may effect clustering and topic modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15817355",
   "metadata": {},
   "source": [
    "#### Did you identify any potientally problematic words?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ad6082",
   "metadata": {},
   "source": [
    "I found some problematic words looking at some of the cleaned data which are as follows:\n",
    "cteure\n",
    "nthern\n",
    "knik\n",
    "chlenge\n",
    "automo\n",
    "ci\n",
    "es\n",
    "There are many such words that don't mean anything."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1507fbe",
   "metadata": {},
   "source": [
    "## Experimenting with Clustering Models\n",
    "\n",
    "Now, you'll start to explore models to find the optimal clustering model. In this section, you'll explore [K-means](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html), [Hierarchical](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html), and [DBSCAN](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html#sklearn.cluster.DBSCAN) clustering algorithms.\n",
    "Create these algorithms with k_clusters for K-means and Hierarchical.\n",
    "For each cell in the table provide the [Silhouette score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html#sklearn.metrics.silhouette_score), [Calinski and Harabasz score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.calinski_harabasz_score.html#sklearn.metrics.calinski_harabasz_score), and [Davies-Bouldin score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.davies_bouldin_score.html#sklearn.metrics.davies_bouldin_score).\n",
    "\n",
    "In each cell, create an array to store the values.\n",
    "For example, \n",
    "\n",
    "|Algorithm| k = 9 | k = 18| k = 36 | Optimal k| \n",
    "|--|--|--|--|--|\n",
    "|K-means| [-0.01, 1.08, 0.95]| [-0.01, 1.07, 0.94] | [-0.001, 1.13, 0.90] | [S,CH,DB] |\n",
    "|Hierarchical |[0.02, 1.49, 2.90]| [-0.02, 1.40, 1.87]| [-0.02, 1.41, 1.21] | [S,CH,DB]|\n",
    "|DBSCAN | X | X | X | [-0.13, 1.11, 2.09] |\n",
    "\n",
    "\n",
    "\n",
    "### Optimality \n",
    "You will need to find the optimal k for K-means and Hierarchical algorithms.\n",
    "Find the optimality for k in the range 2 to 50.\n",
    "Provide the code used to generate the optimal k and provide justification for your approach.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da2b033",
   "metadata": {},
   "source": [
    "|Algorithm| k = 9 | k = 18| k = 36 | Optimal k| \n",
    "|--|--|--|--|--|\n",
    "|K-means|--|--|--|--|\n",
    "|Hierarchical |--|--|--|--|\n",
    "|DBSCAN | X | X | X | -- |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2dc8c7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "import yellowbrick\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b45c5a-2008-4cb1-b7bb-e62f7320b659",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7c84c6-0b86-4df3-9919-20eb63c8289b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(ngram_range=(1,3))\n",
    "tfidfdocs = tfidf.fit_transform(raw_documents=cleaned_text['text'])\n",
    "\n",
    "model = KMeans(n_init='auto' ,max_iter=300)\n",
    "visualizer = KElbowVisualizer(model, k = (2,50), metric = 'silhouette', timings = False)\n",
    "visualizer.fit(tfidfdocs)\n",
    "visualizer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d96d23a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Silhouette Score for k=9 is: -0.014452714836265075\n",
      "The Calinski-Harabasz Score for k=9 is: 1.07613455331852\n",
      "The Davies-Bouldin Score for k=9 is: 0.945060522966189\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(ngram_range=(1,3))\n",
    "tfidfdocs = tfidf.fit_transform(raw_documents=cleaned_text['text'])\n",
    "\n",
    "kmeans = KMeans(n_clusters=9, random_state=42, n_init = 'auto')\n",
    "kmeans.fit_transform(tfidfdocs)\n",
    "\n",
    "s_score = silhouette_score(tfidfdocs, kmeans.labels_)\n",
    "ch_score = calinski_harabasz_score(tfidfdocs.toarray(), kmeans.labels_)\n",
    "db_score = davies_bouldin_score(tfidfdocs.toarray(), kmeans.labels_)\n",
    "\n",
    "print(f\"The Silhouette Score for k=9 is: {s_score}\")\n",
    "print(f\"The Calinski-Harabasz Score for k=9 is: {ch_score}\")\n",
    "print(f\"The Davies-Bouldin Score for k=9 is: {db_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890c15ee-2338-4e88-8b2a-891d39d81e8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1cbec6f4-3d44-4f36-8aca-d1096df012a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Silhouette Score for k=18 is: -0.010334772486326397\n",
      "The Silhouette Score for k=18 is: 1.0666147540091082\n",
      "The Davies-Bouldin Score for k=18 is: 0.938674560096928\n"
     ]
    }
   ],
   "source": [
    "kmeans = KMeans(n_clusters=18, random_state=42, n_init = 'auto')\n",
    "kmeans.fit_transform(tfidfdocs)\n",
    "\n",
    "s_score = silhouette_score(tfidfdocs, kmeans.labels_)\n",
    "ch_score = calinski_harabasz_score(tfidfdocs.toarray(), kmeans.labels_)\n",
    "db_score = davies_bouldin_score(tfidfdocs.toarray(), kmeans.labels_)\n",
    "\n",
    "print(f\"The Silhouette Score for k=18 is: {s_score}\")\n",
    "print(f\"The Silhouette Score for k=18 is: {ch_score}\")\n",
    "print(f\"The Davies-Bouldin Score for k=18 is: {db_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "36c4139f-4e72-4ab2-b764-70aab8781772",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Silhouette Score for k=36 is: -0.0007395182287630532\n",
      "The Silhouette Score for k=36 is: 1.1273725193727573\n",
      "The Davies-Bouldin Score for k=36 is: 0.8961045487403249\n"
     ]
    }
   ],
   "source": [
    "kmeans = KMeans(n_clusters=36, random_state=42, n_init = 'auto')\n",
    "kmeans.fit_transform(tfidfdocs)\n",
    "\n",
    "s_score = silhouette_score(tfidfdocs, kmeans.labels_)\n",
    "ch_score = calinski_harabasz_score(tfidfdocs.toarray(), kmeans.labels_)\n",
    "db_score = davies_bouldin_score(tfidfdocs.toarray(), kmeans.labels_)\n",
    "\n",
    "print(f\"The Silhouette Score for k=36 is: {s_score}\")\n",
    "print(f\"The Silhouette Score for k=36 is: {ch_score}\")\n",
    "print(f\"The Davies-Bouldin Score for k=36 is: {db_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975e32e1-b89f-4b3d-9aa2-b268b79a9945",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Hierarchical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "42480093-166f-4135-8583-19c7a14a7bcd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Silhouette Score for k=9 is: 0.016669680362206243\n",
      "The Calinski-Harabasz Score for k=9 is: 1.4864568669613392\n",
      "The Davies-Bouldin Score for k=9 is: 2.9049682746839576\n"
     ]
    }
   ],
   "source": [
    "agg_clustering = AgglomerativeClustering(n_clusters=9)\n",
    "agg_clustering.fit(tfidfdocs.toarray())\n",
    "\n",
    "s_score = silhouette_score(tfidfdocs, agg_clustering.labels_)\n",
    "ch_score = calinski_harabasz_score(tfidfdocs.toarray(), agg_clustering.labels_)\n",
    "db_score = davies_bouldin_score(tfidfdocs.toarray(), agg_clustering.labels_)\n",
    "\n",
    "print(f\"The Silhouette Score for k=9 is: {s_score}\")\n",
    "print(f\"The Calinski-Harabasz Score for k=9 is: {ch_score}\")\n",
    "print(f\"The Davies-Bouldin Score for k=9 is: {db_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "894ef92d-21d7-442d-92b3-2f835fdf9a0c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Silhouette Score for k=18 is: -0.017012983124648884\n",
      "The Calinski-Harabasz Score for k=18 is: 1.3897788927282986\n",
      "The Davies-Bouldin Score for k=18 is: 1.8690655635557678\n"
     ]
    }
   ],
   "source": [
    "agg_clustering = AgglomerativeClustering(n_clusters=18)\n",
    "agg_clustering.fit(tfidfdocs.toarray())\n",
    "\n",
    "s_score = silhouette_score(tfidfdocs, agg_clustering.labels_)\n",
    "ch_score = calinski_harabasz_score(tfidfdocs.toarray(), agg_clustering.labels_)\n",
    "db_score = davies_bouldin_score(tfidfdocs.toarray(), agg_clustering.labels_)\n",
    "\n",
    "print(f\"The Silhouette Score for k=18 is: {s_score}\")\n",
    "print(f\"The Calinski-Harabasz Score for k=18 is: {ch_score}\")\n",
    "print(f\"The Davies-Bouldin Score for k=18 is: {db_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "65a9535c-df59-418e-bef7-8965fe068889",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Silhouette Score for k=36 is: -0.019914054556347743\n",
      "The Calinski-Harabasz Score for k=36 is: 1.405724107976957\n",
      "The Davies-Bouldin Score for k=36 is: 1.2137268431055608\n"
     ]
    }
   ],
   "source": [
    "agg_clustering = AgglomerativeClustering(n_clusters=36)\n",
    "agg_clustering.fit(tfidfdocs.toarray())\n",
    "\n",
    "s_score = silhouette_score(tfidfdocs, agg_clustering.labels_)\n",
    "ch_score = calinski_harabasz_score(tfidfdocs.toarray(), agg_clustering.labels_)\n",
    "db_score = davies_bouldin_score(tfidfdocs.toarray(), agg_clustering.labels_)\n",
    "\n",
    "print(f\"The Silhouette Score for k=36 is: {s_score}\")\n",
    "print(f\"The Calinski-Harabasz Score for k=36 is: {ch_score}\")\n",
    "print(f\"The Davies-Bouldin Score for k=36 is: {db_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82514e5f-04b3-49e4-8d3e-60c76dda2536",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f86a100f-f845-4133-9ecf-04ff0eecf907",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Silhouette Score for DBSCAN is: -0.1327183959000816\n",
      "The Calinski-Harabasz Score for DBSCAN is: 1.111580058716288\n",
      "The Davies-Bouldin Score for DBSCAN is: 2.0872961354719326\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "\n",
    "# Perform PCA to reduce the dimensionality of the dataset\n",
    "pca = PCA(n_components=69)\n",
    "tfidf_pca = pca.fit_transform(tfidfdocs.toarray())\n",
    "\n",
    "# Apply DBSCAN to the reduced dataset\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
    "dbscan.fit(tfidf_pca)\n",
    "\n",
    "# Calculate the evaluation metrics\n",
    "s_score = silhouette_score(tfidf_pca, dbscan.labels_)\n",
    "ch_score = calinski_harabasz_score(tfidf_pca, dbscan.labels_)\n",
    "db_score = davies_bouldin_score(tfidf_pca, dbscan.labels_)\n",
    "\n",
    "print(f\"The Silhouette Score for DBSCAN is: {s_score}\")\n",
    "print(f\"The Calinski-Harabasz Score for DBSCAN is: {ch_score}\")\n",
    "print(f\"The Davies-Bouldin Score for DBSCAN is: {db_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca04e7ba-b43d-42ec-8f2b-93940dc1f7f3",
   "metadata": {},
   "source": [
    "### Optimal K for KMeans and Hierarchical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b4d8c04d-fc55-4108-af44-da81572869f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "def find_optimal_k(dataset, k_range):\n",
    "    # Perform PCA to reduce the dimensionality of the dataset\n",
    "    pca = PCA(n_components=min(dataset.shape)-1)\n",
    "    dataset_pca = pca.fit_transform(dataset)\n",
    "    \n",
    "    # Initialize lists to store evaluation scores for K-means and hierarchical clustering\n",
    "    kmeans_scores = []\n",
    "    hier_scores = []\n",
    "    \n",
    "    # Iterate over k values and perform clustering for both K-means and hierarchical clustering\n",
    "    for k in k_range:\n",
    "        # K-means clustering\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42, n_init = 'auto')\n",
    "        kmeans.fit(dataset_pca)\n",
    "        kmeans_silhouette = silhouette_score(dataset_pca, kmeans.labels_)\n",
    "        kmeans_ch = calinski_harabasz_score(dataset_pca, kmeans.labels_)\n",
    "        kmeans_db = davies_bouldin_score(dataset_pca, kmeans.labels_)\n",
    "        kmeans_scores.append([k, kmeans_silhouette, kmeans_ch, kmeans_db])\n",
    "\n",
    "        # Hierarchical clustering\n",
    "        hier = AgglomerativeClustering(n_clusters=k)\n",
    "        hier.fit(dataset_pca)\n",
    "        hier_silhouette = silhouette_score(dataset_pca, hier.labels_)\n",
    "        hier_ch = calinski_harabasz_score(dataset_pca, hier.labels_)\n",
    "        hier_db = davies_bouldin_score(dataset_pca, hier.labels_)\n",
    "        hier_scores.append([k, hier_silhouette, hier_ch, hier_db])\n",
    "\n",
    "    # Convert scores to numpy arrays for easier manipulation\n",
    "    kmeans_scores = np.array(kmeans_scores)\n",
    "    hier_scores = np.array(hier_scores)\n",
    "\n",
    "    # Find optimal k value for K-means clustering based on highest Silhouette score\n",
    "    kmeans_optimal = kmeans_scores[np.argmax(kmeans_scores[:, 1]), 0]\n",
    "\n",
    "    # Find optimal k value for hierarchical clustering based on highest Silhouette score\n",
    "    hier_optimal = hier_scores[np.argmax(hier_scores[:, 1]), 0]\n",
    "\n",
    "    # Return optimal k values along with evaluation scores for both K-means and hierarchical clustering\n",
    "    return kmeans_optimal, hier_optimal, kmeans_scores, hier_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1877f816-6652-456c-ba10-48763b54f26a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-means optimal k: 49.0\n",
      "Hierarchical optimal k: 4.0\n",
      "K-means scores:\n",
      " [[ 2.00000000e+00 -5.55333211e-02  1.57096383e+00  3.30650636e+00]\n",
      " [ 3.00000000e+00 -7.23233737e-02  1.34034281e+00  2.75160469e+00]\n",
      " [ 4.00000000e+00 -7.12341096e-02  1.27035757e+00  2.62627181e+00]\n",
      " [ 5.00000000e+00 -6.57352242e-02  1.34821161e+00  2.45642936e+00]\n",
      " [ 6.00000000e+00 -6.36998591e-02  1.29453768e+00  2.20884845e+00]\n",
      " [ 7.00000000e+00 -6.09713638e-02  1.28297800e+00  2.01928025e+00]\n",
      " [ 8.00000000e+00 -1.49333709e-02  1.06634512e+00  9.50428621e-01]\n",
      " [ 9.00000000e+00 -1.47708751e-02  1.05292366e+00  9.54213104e-01]\n",
      " [ 1.00000000e+01 -1.45838469e-02  1.04710105e+00  9.55101620e-01]\n",
      " [ 1.10000000e+01 -1.42978931e-02  1.05386360e+00  9.50960780e-01]\n",
      " [ 1.20000000e+01 -1.33078110e-02  1.06380324e+00  9.45902298e-01]\n",
      " [ 1.30000000e+01 -1.27400850e-02  1.05796434e+00  9.47046936e-01]\n",
      " [ 1.40000000e+01 -1.30255856e-02  1.05355138e+00  9.47916759e-01]\n",
      " [ 1.50000000e+01 -1.25004896e-02  1.05032487e+00  9.48184047e-01]\n",
      " [ 1.60000000e+01 -1.16486968e-02  1.06701280e+00  9.41260515e-01]\n",
      " [ 1.70000000e+01 -1.08988586e-02  1.07274817e+00  9.38024919e-01]\n",
      " [ 1.80000000e+01 -1.00123175e-02  1.07214351e+00  9.37125310e-01]\n",
      " [ 1.90000000e+01 -8.73034965e-03  1.08048157e+00  9.32891019e-01]\n",
      " [ 2.00000000e+01 -8.36483431e-03  1.07973805e+00  9.32234630e-01]\n",
      " [ 2.10000000e+01 -7.46161420e-03  1.10174211e+00  9.23586098e-01]\n",
      " [ 2.20000000e+01 -6.81634817e-03  1.10271076e+00  9.22147834e-01]\n",
      " [ 2.30000000e+01 -6.13149326e-03  1.10356958e+00  9.20729796e-01]\n",
      " [ 2.40000000e+01 -5.36993781e-03  1.10384468e+00  9.19503068e-01]\n",
      " [ 2.50000000e+01 -4.72900218e-03  1.10550566e+00  9.17752398e-01]\n",
      " [ 2.60000000e+01 -4.05142184e-03  1.10436189e+00  9.16845027e-01]\n",
      " [ 2.70000000e+01 -2.90466523e-03  1.12185312e+00  9.10133058e-01]\n",
      " [ 2.80000000e+01 -6.81227163e-03  1.11358421e+00  9.12758983e-01]\n",
      " [ 2.90000000e+01 -6.09448199e-03  1.11879045e+00  9.09567671e-01]\n",
      " [ 3.00000000e+01 -5.28874895e-03  1.12452610e+00  9.06275491e-01]\n",
      " [ 3.10000000e+01 -4.47783461e-03  1.12396318e+00  9.05057388e-01]\n",
      " [ 3.20000000e+01 -3.48146645e-03  1.12573675e+00  9.02856109e-01]\n",
      " [ 3.30000000e+01 -2.07999653e-03  1.15486682e+00  8.93262021e-01]\n",
      " [ 3.40000000e+01 -1.12543598e-03  1.15589898e+00  8.91359339e-01]\n",
      " [ 3.50000000e+01  2.57002990e-05  1.15769313e+00  8.89014895e-01]\n",
      " [ 3.60000000e+01  7.76685889e-04  1.16236945e+00  8.85649635e-01]\n",
      " [ 3.70000000e+01  1.85497941e-03  1.16518909e+00  8.82778633e-01]\n",
      " [ 3.80000000e+01  2.16401487e-03  1.16291378e+00  8.81557732e-01]\n",
      " [ 3.90000000e+01  3.18229000e-03  1.17382984e+00  8.76274378e-01]\n",
      " [ 4.00000000e+01  4.17571484e-03  1.17436025e+00  8.73707531e-01]\n",
      " [ 4.10000000e+01  4.91402124e-03  1.17095986e+00  8.72004313e-01]\n",
      " [ 4.20000000e+01  5.96458766e-03  1.18237225e+00  8.66091778e-01]\n",
      " [ 4.30000000e+01  6.05839974e-03  1.17891746e+00  8.63816130e-01]\n",
      " [ 4.40000000e+01  7.03250206e-03  1.19357151e+00  8.56928823e-01]\n",
      " [ 4.50000000e+01  1.01929410e-02  1.20819822e+00  8.49462609e-01]\n",
      " [ 4.60000000e+01  1.15170384e-02  1.22504013e+00  8.41567240e-01]\n",
      " [ 4.70000000e+01  1.27406658e-02  1.23139580e+00  8.35468072e-01]\n",
      " [ 4.80000000e+01  1.37303350e-02  1.24311907e+00  8.28084251e-01]\n",
      " [ 4.90000000e+01  1.54722122e-02  1.24908591e+00  8.20878815e-01]]\n",
      "Hierarchical scores:\n",
      " [[ 2.00000000e+00  2.10648214e-02  2.11772823e+00  5.36034785e+00]\n",
      " [ 3.00000000e+00  2.46330349e-02  1.82740927e+00  4.07644390e+00]\n",
      " [ 4.00000000e+00  2.60331280e-02  1.69738975e+00  4.24188422e+00]\n",
      " [ 5.00000000e+00  2.40230302e-02  1.62856064e+00  3.55534176e+00]\n",
      " [ 6.00000000e+00  1.87024315e-02  1.57598913e+00  3.36649496e+00]\n",
      " [ 7.00000000e+00  1.75992366e-02  1.54111346e+00  3.13996877e+00]\n",
      " [ 8.00000000e+00  1.87107364e-02  1.51149485e+00  3.17112767e+00]\n",
      " [ 9.00000000e+00  1.66696803e-02  1.48645687e+00  2.90496827e+00]\n",
      " [ 1.00000000e+01  1.44111747e-02  1.46792418e+00  2.69318947e+00]\n",
      " [ 1.10000000e+01  1.14925681e-02  1.45237298e+00  2.52033998e+00]\n",
      " [ 1.20000000e+01  1.28090382e-02  1.44039882e+00  2.34189791e+00]\n",
      " [ 1.30000000e+01  1.03825699e-02  1.42839608e+00  2.22153399e+00]\n",
      " [ 1.40000000e+01  7.57581317e-03  1.41753274e+00  2.11787367e+00]\n",
      " [ 1.50000000e+01  4.24485217e-03  1.40900125e+00  2.02772584e+00]\n",
      " [ 1.60000000e+01  1.03468486e-03  1.40169756e+00  1.94798100e+00]\n",
      " [ 1.70000000e+01 -1.05252783e-02  1.39574476e+00  1.93853989e+00]\n",
      " [ 1.80000000e+01 -1.70129832e-02  1.38977889e+00  1.86906556e+00]\n",
      " [ 1.90000000e+01 -1.59184207e-02  1.38524588e+00  1.85549482e+00]\n",
      " [ 2.00000000e+01 -2.30825647e-02  1.38153603e+00  1.79534279e+00]\n",
      " [ 2.10000000e+01 -2.09881467e-02  1.37890178e+00  1.74120114e+00]\n",
      " [ 2.20000000e+01 -1.69110471e-02  1.37728934e+00  1.64626478e+00]\n",
      " [ 2.30000000e+01 -1.30050408e-02  1.37623173e+00  1.56341355e+00]\n",
      " [ 2.40000000e+01 -2.21154588e-02  1.37591866e+00  1.52409798e+00]\n",
      " [ 2.50000000e+01 -3.23920184e-02  1.37547193e+00  1.48394117e+00]\n",
      " [ 2.60000000e+01 -2.96016020e-02  1.37578160e+00  1.45036675e+00]\n",
      " [ 2.70000000e+01 -4.28426928e-02  1.37687785e+00  1.40467650e+00]\n",
      " [ 2.80000000e+01 -4.24951518e-02  1.37855348e+00  1.41571476e+00]\n",
      " [ 2.90000000e+01 -4.22131448e-02  1.38029424e+00  1.42224854e+00]\n",
      " [ 3.00000000e+01 -3.90005186e-02  1.38237856e+00  1.39164604e+00]\n",
      " [ 3.10000000e+01 -3.87572011e-02  1.38513435e+00  1.39676104e+00]\n",
      " [ 3.20000000e+01 -3.55472178e-02  1.38822575e+00  1.36339024e+00]\n",
      " [ 3.30000000e+01 -3.23438156e-02  1.39198487e+00  1.32917234e+00]\n",
      " [ 3.40000000e+01 -3.21231197e-02  1.39580451e+00  1.30445093e+00]\n",
      " [ 3.50000000e+01 -2.60109302e-02  1.40035938e+00  1.25719610e+00]\n",
      " [ 3.60000000e+01 -1.99140546e-02  1.40572411e+00  1.21372684e+00]\n",
      " [ 3.70000000e+01 -1.68036493e-02  1.41132396e+00  1.19552948e+00]\n",
      " [ 3.80000000e+01 -1.08568137e-02  1.41751737e+00  1.15610656e+00]\n",
      " [ 3.90000000e+01 -1.05917846e-02  1.42469100e+00  1.16559783e+00]\n",
      " [ 4.00000000e+01 -7.51259412e-03  1.43275791e+00  1.15085056e+00]\n",
      " [ 4.10000000e+01 -4.50089867e-03  1.44200979e+00  1.13542765e+00]\n",
      " [ 4.20000000e+01 -1.45926520e-03  1.45177545e+00  1.11751278e+00]\n",
      " [ 4.30000000e+01  1.52130148e-03  1.46289477e+00  1.08926598e+00]\n",
      " [ 4.40000000e+01  2.16012177e-03  1.47466033e+00  1.09669102e+00]\n",
      " [ 4.50000000e+01  5.01552348e-03  1.48819307e+00  1.08350939e+00]\n",
      " [ 4.60000000e+01  7.94489787e-03  1.50375504e+00  1.05183156e+00]\n",
      " [ 4.70000000e+01  1.08743218e-02  1.52096920e+00  1.03716863e+00]\n",
      " [ 4.80000000e+01  1.63769433e-02  1.54047954e+00  1.00905850e+00]\n",
      " [ 4.90000000e+01  1.91237417e-02  1.56277854e+00  9.96212863e-01]]\n"
     ]
    }
   ],
   "source": [
    "k_range = range(2, 50)\n",
    "kmeans_optimal, hier_optimal, kmeans_scores, hier_scores = find_optimal_k(tfidfdocs.toarray(), k_range)\n",
    "\n",
    "print(\"K-means optimal k:\", kmeans_optimal)\n",
    "print(\"Hierarchical optimal k:\", hier_optimal)\n",
    "print(\"K-means scores:\\n\", kmeans_scores)\n",
    "print(\"Hierarchical scores:\\n\", hier_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b98d403d-7198-49c4-85da-12929fe47584",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "def find_optimal_k(dataset, k_range):\n",
    "    # Perform PCA to reduce the dimensionality of the dataset\n",
    "    pca = PCA(n_components=min(dataset.shape)-1)\n",
    "    dataset_pca = pca.fit_transform(dataset)\n",
    "    \n",
    "    # Initialize lists to store evaluation scores for K-means and hierarchical clustering\n",
    "    kmeans_scores = []\n",
    "    hier_scores = []\n",
    "    \n",
    "    # Iterate over k values and perform clustering for both K-means and hierarchical clustering\n",
    "    for k in k_range:\n",
    "        # K-means clustering\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42, n_init = 'auto')\n",
    "        kmeans.fit(dataset_pca)\n",
    "        kmeans_silhouette = silhouette_score(dataset_pca, kmeans.labels_)\n",
    "        kmeans_ch = calinski_harabasz_score(dataset_pca, kmeans.labels_)\n",
    "        kmeans_db = davies_bouldin_score(dataset_pca, kmeans.labels_)\n",
    "        kmeans_scores.append([k, kmeans_silhouette, kmeans_ch, kmeans_db])\n",
    "\n",
    "        # Hierarchical clustering\n",
    "        hier = AgglomerativeClustering(n_clusters=k)\n",
    "        hier.fit(dataset_pca)\n",
    "        hier_silhouette = silhouette_score(dataset_pca, hier.labels_)\n",
    "        hier_ch = calinski_harabasz_score(dataset_pca, hier.labels_)\n",
    "        hier_db = davies_bouldin_score(dataset_pca, hier.labels_)\n",
    "        hier_scores.append([k, hier_silhouette, hier_ch, hier_db])\n",
    "\n",
    "    # Convert scores to numpy arrays for easier manipulation\n",
    "    kmeans_scores = np.array(kmeans_scores)\n",
    "    hier_scores = np.array(hier_scores)\n",
    "\n",
    "    # Find optimal k value for K-means clustering based on highest Silhouette score\n",
    "    kmeans_optimal = kmeans_scores[np.argmax(kmeans_scores[:, 1]), 0]\n",
    "\n",
    "    # Find optimal k value for hierarchical clustering based on highest Silhouette score\n",
    "    hier_optimal = hier_scores[np.argmax(hier_scores[:, 1]), 0]\n",
    "\n",
    "    # Get the kmeans scores of the optimal k\n",
    "    kmeans_optimal_scores = kmeans_scores[np.argmax(kmeans_scores[:, 1])]\n",
    "\n",
    "    # Get the hierarchical scores of the optimal k\n",
    "    hier_optimal_scores = hier_scores[np.argmax(hier_scores[:, 1])]\n",
    "\n",
    "    # Return optimal k values along with evaluation scores for both K-means and hierarchical clustering\n",
    "    return kmeans_optimal, hier_optimal, kmeans_optimal_scores, hier_optimal_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f9622348-ef30-4f77-8f52-c455b0561ac8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-means optimal k: 9.0\n",
      "Hierarchical optimal k: 4.0\n",
      "K-means scores:\n",
      " [ 9.         -0.01477088  1.05292366  0.9542131 ]\n",
      "Hierarchical scores:\n",
      " [4.         0.02603313 1.69738975 4.24188422]\n"
     ]
    }
   ],
   "source": [
    "k_range = range(2, 10)\n",
    "kmeans_optimal, hier_optimal, kmeans_scores, hier_scores = find_optimal_k(tfidfdocs.toarray(), k_range)\n",
    "\n",
    "print(\"K-means optimal k:\", kmeans_optimal)\n",
    "print(\"Hierarchical optimal k:\", hier_optimal)\n",
    "print(\"K-means scores:\\n\", kmeans_scores)\n",
    "print(\"Hierarchical scores:\\n\", hier_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c20fd69",
   "metadata": {},
   "source": [
    "#### How did you approach finding the optimal k?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753c54da",
   "metadata": {},
   "source": [
    "[Your answer here]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79ec635",
   "metadata": {},
   "source": [
    "#### What algorithm do you believe is the best? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3136b0a4",
   "metadata": {},
   "source": [
    "[Your Answer]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e45a2a3",
   "metadata": {},
   "source": [
    "### Add Cluster ID to output file\n",
    "In your data structure, add the cluster id for each smart city respectively. Show the to append the clusterid code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad83e3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "959e7275",
   "metadata": {},
   "source": [
    "### Save Model\n",
    "\n",
    "After finding the best model, it is desirable to have a way to persist the model for future use without having to retrain. Save the model using [model persistance](https://scikit-learn.org/stable/model_persistence.html). This model should be saved in the same directory as this notebook and should be loaded as the model for your `project3.py`.\n",
    "\n",
    "Save the model as `model.pkl`. You do not have to use pickle, but be sure to save the persistance using one of the methods listed in the link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c80938e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3fe5a0c9",
   "metadata": {},
   "source": [
    "## Derving Themes and Concepts\n",
    "\n",
    "Perform Topic Modeling on the cleaned data. Provide the top five words for `TOPIC_NUM = Best_k` as defined in the section above. Feel free to reference [Chapter 6](https://github.com/dipanjanS/text-analytics-with-python/tree/master/New-Second-Edition/Ch06%20-%20Text%20Summarization%20and%20Topic%20Models) for more information on Topic Modeling and Summarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b684bc24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915516dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2a3896",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1573fe65",
   "metadata": {},
   "source": [
    "### Extract themes\n",
    "Write a theme for each topic (atleast a sentence each)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9737d8df",
   "metadata": {},
   "source": [
    "[Your Answer]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21cf24c6",
   "metadata": {},
   "source": [
    "[Your Answer]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e191f4",
   "metadata": {},
   "source": [
    "[Your Answer]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ab7df0",
   "metadata": {},
   "source": [
    "### Add Topid ID to output file\n",
    "Add the top two topics for each smart city to the data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e937841",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e568652",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "39f9c240",
   "metadata": {},
   "source": [
    "## Gathering Applicant Summaries and Keywords\n",
    "\n",
    "For each smart city applicant, gather a summary and keywords that are important to that document. You can use gensim to do this. Here are examples of functions that you could use.\n",
    "\n",
    "```python\n",
    "\n",
    "from gensim.summarization import summarize\n",
    "\n",
    "def summary(text, ratio=0.2, word_count=250, split=False):\n",
    "    return summarize(text, ratio= ratio, word_count=word_count, split=split)\n",
    "    \n",
    "from gensim.summarization import keywords\n",
    "\n",
    "def keys(text, ratio=0.01):\n",
    "    return keywords(text, ratio=ratio)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86cbe26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b545ac4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2d27ce37",
   "metadata": {},
   "source": [
    "### Add Summaries and Keywords\n",
    "Add summary and keywords to output file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09357ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4f9cb2c4",
   "metadata": {},
   "source": [
    "## Write output data\n",
    "\n",
    "The output data should be written as a TSV file.\n",
    "You can use `to_csv` method from Pandas for this if you are using a DataFrame.\n",
    "\n",
    "`Syntax: df.to_csv('file.tsv', sep = '')` \\\n",
    "`df.to_csv('smartcity_eda.tsv', sep='\\t')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58827464",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7a18b8ff",
   "metadata": {},
   "source": [
    "# Moving Forward\n",
    "Now that you have explored the dataset, take the important features and functions to create your `project3.py`.\n",
    "Please refer to the project spec for more guidance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6675ba",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
