{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2342813f",
   "metadata": {},
   "source": [
    "## Student Name: Shiv Patel\n",
    "## Student Email: shiv.s.patel@ou.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e6ab65",
   "metadata": {},
   "source": [
    "# Project 3: The Smart City Slicker\n",
    "\n",
    "Imagine you are a stakeholder in a rising Smart City and want to know more about themes and concepts about existing smart cities. You also want to know where does your smart city place among others. In this project, you will perform \n",
    "exploratory data analysis, often shortened to EDA, to examine a data from the [2015 Smart City Challenge](https://www.transportation.gov/smartcity) to find facts about the data and communicating those facts through text analysis and visualizations.\n",
    "\n",
    "In order to explore the data and visualize it, some modifications might need to be made to the data along the way. This is often referred to as data preprocessing or cleaning.\n",
    "Though data preprocessing is technically different from EDA, EDA often exposes problems with the data that need to be fixed in order to continue exploring.\n",
    "Because of this tight coupling, you have to clean the data as necessary to help understand the data.\n",
    "\n",
    "In this project, you will apply your knowledge about data cleaning, machine learning, visualizations, and databases to explore smart city applications.\n",
    "\n",
    "**Part 1** of the notebook will explore and clean the data. \\\n",
    "**Part 2** will take the results of the preprocessed data to create models and visualizations.\n",
    "\n",
    "Empty cells are code cells. \n",
    "Cells denoted with [Your Answer Here] are markdown cells.\n",
    "Edit and add as many cells as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8dcba1",
   "metadata": {},
   "source": [
    "Output file for this notebook is shown as a table for display purposes. Note: The city name can be Norman, OK or OK Norman.\n",
    "\n",
    "| city | raw text | clean text | clusterid | topicids | summary | keywords|\n",
    "| -- | -- | -- | -- | -- | -- | -- |\n",
    "|Norman, OK | Test, test , and testing. | test test test | 0 | T1, T2| test | test |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fd47ce",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "The Dataset: 2015 Smart City Challenge Applicants (non-finalist).\n",
    "In this project you will use the applicant's PDFs as a dataset.\n",
    "The dataset is from the U.S Department of Transportation Smart City Challenge.\n",
    "\n",
    "On the website page for the data, you can find some basic information about the challenge. This is an interesting dataset. Think of the questions that you might be able to answer! A few could be:\n",
    "\n",
    "1. Can I identify frequently occurring words that could be removed during data preprocessing?\n",
    "2. Where are the applicants from?\n",
    "3. Are there multiple entries for the same city in different applicantions?\n",
    "4. What are the major themes and concepts from the smart city applicants?\n",
    "\n",
    "Let's load the data!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aace966",
   "metadata": {},
   "source": [
    "## Loading and Handling files\n",
    "\n",
    "Load data from `smartcity/`. \n",
    "\n",
    "To extract the data from the pdf files, use the [pypdf.pdf.PdfFileReader](https://pypdf.readthedocs.io/en/stable/index.html) class.\n",
    "It will allow you to extract pages and pdf files and add them to a data structure (dataframe, list, dictionary, etc).\n",
    "To install the module, use the command `pipenv install pypdf`.\n",
    "You only need to handle PDF files, handling docx is not necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "id": "151c5a06-9d74-41b9-aa8e-90be00607b2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get the pdf file names given in the smartcity folder\n",
    "from pypdf import PdfReader\n",
    "import os\n",
    "folderPath = 'smartcity/'\n",
    "\n",
    "pdf_files = [f for f in os.listdir(folderPath) if f.endswith('.pdf')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ed6e32",
   "metadata": {},
   "source": [
    "Create a data structure to add the city name and raw text. You can choose to split the city name from the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2e4905f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract the data from all the pdfs, store the city names under city column of the dataFrame and the extracted text as raw text\n",
    "import pandas as pd\n",
    "dataFrame = pd.DataFrame(columns=['city', 'raw text'])\n",
    "for name in pdf_files:\n",
    "    x = name.endswith(\".pdf\")\n",
    "    if x == True:\n",
    "        filePath = 'smartcity/' + name\n",
    "        reader = PdfReader(filePath)\n",
    "        extracted_text = []\n",
    "\n",
    "        # Iterate through each page in the PDF file\n",
    "        for page_num in range(len(reader.pages)):\n",
    "\n",
    "            page = reader.pages[page_num]\n",
    "            # Extract the text from the page\n",
    "            text = page.extract_text()\n",
    "\n",
    "            # Add the extracted text to the list\n",
    "            extracted_text.append(text)\n",
    "        name = name[:-4]\n",
    "        data_dict = {'city': name, 'raw text': extracted_text}\n",
    "    dataFrame = pd.concat([dataFrame, pd.DataFrame(data_dict)], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "id": "583fa9a6-8e59-4a3d-91e0-a54c57f839b3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 city                                           raw text\n",
      "0        AK Anchorage    CONTENTS \\n1 VISION ...........................\n",
      "1       AL Birmingham  aBirmingham\\nRising\\nBirmingham Rising! Meetin...\n",
      "2       AL Montgomery   \\n \\n U.S. Department of Transportation - “BE...\n",
      "3    AZ Scottsdale AZ    \\n  \\n \\n \\n \\nFederal Agency Name:   U.S. D...\n",
      "4           AZ Tucson  Tucson Smart City Demonstration Proposal\\nPart...\n",
      "..                ...                                                ...\n",
      "64        VA Richmond    \\n \\n \\n   \\n \\n \\n  \\n      Contact Informa...\n",
      "65  VA Virginia Beach    \\n 1.  Project Vision  ........................\n",
      "66         WA Seattle  Beyond Traffic: USDOT Smart City Challenge\\nAp...\n",
      "67         WA Spokane  USDOT Smart City Challenge -  Spokane  \\nPage ...\n",
      "68         WI Madison  Building a Smart Madison  \\nfor Shared Prosper...\n",
      "\n",
      "[69 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# group the rows by filename and combine the text column\n",
    "dataFrame = dataFrame.groupby(\"city\")[\"raw text\"].apply(lambda x: \" \".join(x)).reset_index()\n",
    "print(dataFrame)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5019a8c3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Cleaning Up PDFs\n",
    "\n",
    "One of the more frustrating aspects of PDF is loading the data into a readable format. The first order of business will be to preprocess the data. To start, you can use code provided by Text Analytics with Python, [Chapter 3](https://github.com/dipanjanS/text-analytics-with-python/blob/master/New-Second-Edition/Ch03%20-%20Processing%20and%20Understanding%20Text/Ch03a%20-%20Text%20Wrangling.ipynb): [contractions.py](https://github.com/dipanjanS/text-analytics-with-python/blob/master/New-Second-Edition/Ch05%20-%20Text%20Classification/contractions.py) (Pages 136-137), and [text_normalizer.py](https://github.com/dipanjanS/text-analytics-with-python/blob/master/New-Second-Edition/Ch05%20-%20Text%20Classification/text_normalizer.py) (Pages 155-156). Feel free to download the scripts or add the code directly to the notebook (please note this code is performed on dataframes).\n",
    "\n",
    "In addition to the data cleaning provided by the textbook, you will need to:\n",
    "1. Consider removing terms that may effect clustering and topic modeling. Words to consider are cities, states, common words (smart, city, page, etc.). Keep in mind n-gram combinations are important; this can also be revisited later depending on your model's performance.\n",
    "2. Check the data to remove applicants that text was not processed correctly. Do not remove more than 15 cities from the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "id": "8142e498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# used the code given in text_normalizer.py\n",
    "import nltk\n",
    "import spacy\n",
    "import unicodedata\n",
    "import re\n",
    "from nltk.corpus import wordnet\n",
    "import collections\n",
    "#from textblob import Word\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "tokenizer = ToktokTokenizer()\n",
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "# nlp_vec = spacy.load('en_vectors_web_lg', parse=True, tag=True, entity=True)\n",
    "\n",
    "CONTRACTION_MAP = {\n",
    "\"ain't\": \"is not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he'll've\": \"he he will have\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"I'd\": \"I would\",\n",
    "\"I'd've\": \"I would have\",\n",
    "\"I'll\": \"I will\",\n",
    "\"I'll've\": \"I will have\",\n",
    "\"I'm\": \"I am\",\n",
    "\"I've\": \"I have\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'd've\": \"i would have\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'll've\": \"i will have\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it'll've\": \"it will have\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she'll've\": \"she will have\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so as\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they'll've\": \"they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what'll've\": \"what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who'll've\": \"who will have\",\n",
    "\"who's\": \"who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you'll've\": \"you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}\n",
    "\n",
    "def strip_html_tags(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    if bool(soup.find()):\n",
    "        [s.extract() for s in soup(['iframe', 'script'])]\n",
    "        stripped_text = soup.get_text()\n",
    "        stripped_text = re.sub(r'[\\r|\\n|\\r\\n]+', '\\n', stripped_text)\n",
    "    else:\n",
    "        stripped_text = text\n",
    "    return stripped_text\n",
    "\n",
    "\n",
    "#def correct_spellings_textblob(tokens):\n",
    "#\treturn [Word(token).correct() for token in tokens]  \n",
    "\n",
    "\n",
    "def simple_porter_stemming(text):\n",
    "    ps = nltk.porter.PorterStemmer()\n",
    "    text = ' '.join([ps.stem(word) for word in text.split()])\n",
    "    return text\n",
    "\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    text = nlp(text)\n",
    "    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_repeated_characters(tokens):\n",
    "    repeat_pattern = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
    "    match_substitution = r'\\1\\2\\3'\n",
    "    def replace(old_word):\n",
    "        if wordnet.synsets(old_word):\n",
    "            return old_word\n",
    "        new_word = repeat_pattern.sub(match_substitution, old_word)\n",
    "        return replace(new_word) if new_word != old_word else new_word\n",
    "            \n",
    "    correct_tokens = [replace(word) for word in tokens]\n",
    "    return correct_tokens\n",
    "\n",
    "\n",
    "def expand_contractions(text, contraction_mapping=CONTRACTION_MAP):\n",
    "    \n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
    "                                      flags=re.IGNORECASE|re.DOTALL)\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match)\\\n",
    "                                if contraction_mapping.get(match)\\\n",
    "                                else contraction_mapping.get(match.lower())                       \n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "        \n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    return expanded_text\n",
    "\n",
    "\n",
    "def remove_accented_chars(text):\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_special_characters(text, remove_digits=False):\n",
    "    pattern = r'[^a-zA-Z0-9\\s]|\\[|\\]' if not remove_digits else r'[^a-zA-Z\\s]|\\[|\\]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_stopwords(text, is_lower_case=False, stopwords=stopword_list):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stopwords]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopwords]\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text\n",
    "\n",
    "# Removes the states name\n",
    "def remove_States(text):\n",
    "    pattern = r'Alabama|Alaska|Arizona|Arkansas|California|Colorado|Connecticut|Delaware|Florida|Georgia|Hawaii|Idaho|Illinois|Indiana|Iowa|Kansas|Kentucky|Louisiana|Maine|Maryland|Massachusetts|Michigan|Minnesota|Mississippi|Missouri|Montana|Nebraska|Nevada|New\\sHampshire|New\\sJersey|New\\sMexico|New\\sYork|North\\sCarolina|North\\sDakota|Ohio|Oklahoma|Oregon|Pennsylvania|Rhode\\sIsland|South\\sCarolina|South\\sDakota|Tennessee|Texas|Utah|Vermont|Virginia|Washington|West\\sVirginia|Wisconsin|Wyoming|AL|AK|AZ|AR|CA|CO|CT|DE|FL|GA|HI|ID|IL|IN|IA|KS|KY|LA|ME|MD|MA|MI|MN|MS|MO|MT|NE|NV|NH|NJ|NM|NY|NC|ND|OH|OK|OR|PA|RI|SC|SD|TN|TX|UT|VT|VA|WA|WV|WI|WY'                        \n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text\n",
    "\n",
    "# Removes the common words which I think are not that important in the pdf files\n",
    "def remove_CommonWords(text):\n",
    "    pattern = r\"U\\.S\\. Department|use|traffic|datum|system|smartcity|smart|city|page|section|element|concept|appendix|transportation|(\\s(a|b|c|d|e|f|g|h|i|j|k|l|m|n|o|p|q|r|s|t|u|v|w|x|y|z)\\s)|(\\s\\w\\w\\s)\"\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text\n",
    "\n",
    "# Removes the city names from the pdf files\n",
    "def remove_Cities(text):\n",
    "    pattern = r'Anchorage|Birmingham|Montgomery|Scottsdale|Tucson|Chula Vista|Fremont|Fresno|Long Beach|Moreno Valley|Oakland|Oceanside|Riverside|Sacramento|San Jose_0|NewHaven|DC_0|Jacksonville|Miami|Orlando|St. Petersburg|Tallahassee|Tampa|Atlanta|Brookhaven|Des Moines|Indianapolis|Louisville|Baton Rogue|New Orleans|Shreveport|Boston|Baltimore|Detroit|Port Huron and Marysville|Minneapolis St Paul|St. Louis|Charlotte|Greensboro|Raleigh|Lincoln|Omaha|Jersey City|Newark|Las Vegas|Reno|Albany Troy Schenectady Saratoga Springs|Buffalo|Mt Vernon Yonkers New Rochelle|Rochester|Akron|Canton|Cleveland|Toledo|Oklahoma City|Tulsa|Providence|Greenville|Chattanooga|Memphis|Nashville|Lubbock|Newport News|Norfolk|Richmond|Virginia Beach|Seattle|Spokane|Madison'                        \n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text\n",
    "\n",
    "def normalize_corpus(corpus, html_stripping=True, contraction_expansion=True,\n",
    "                     accented_char_removal=True, text_lower_case=True, \n",
    "                     text_stemming=True, text_lemmatization=True, \n",
    "                     special_char_removal=True, remove_digits=True,\n",
    "                     stopword_removal=True, removeStates = True, removeCities = True,\n",
    "                     removeCommonWords = True, stopwords=stopword_list):\n",
    "    \n",
    "    normalized_corpus = []\n",
    "    # normalize each document in the corpus\n",
    "    for doc in corpus:\n",
    "\n",
    "        # strip HTML\n",
    "        if html_stripping:\n",
    "            doc = strip_html_tags(doc)\n",
    "\n",
    "        # remove extra newlines\n",
    "        doc = doc.translate(doc.maketrans(\"\\n\\t\\r\", \"   \"))\n",
    "\n",
    "        # remove accented characters\n",
    "        if accented_char_removal:\n",
    "            doc = remove_accented_chars(doc)\n",
    "\n",
    "        # expand contractions    \n",
    "        if contraction_expansion:\n",
    "            doc = expand_contractions(doc)\n",
    "\n",
    "        # lemmatize text\n",
    "        if text_lemmatization:\n",
    "            doc = lemmatize_text(doc)\n",
    "\n",
    "        # stem text\n",
    "        if text_stemming and not text_lemmatization:\n",
    "        \tdoc = simple_porter_stemming(doc)\n",
    "\n",
    "        # remove special characters and\\or digits    \n",
    "        if special_char_removal:\n",
    "            # insert spaces between special characters to isolate them    \n",
    "            special_char_pattern = re.compile(r'([{.(-)!}])')\n",
    "            doc = special_char_pattern.sub(\" \\\\1 \", doc)\n",
    "            doc = remove_special_characters(doc, remove_digits=remove_digits)  \n",
    "\n",
    "        # remove extra whitespace\n",
    "        doc = re.sub(' +', ' ', doc)\n",
    "\n",
    "         # remove states\n",
    "        if removeStates:\n",
    "            doc = remove_States(doc)\n",
    "            \n",
    "        # remove cities\n",
    "        if removeCities:\n",
    "            doc = remove_Cities(doc)\n",
    "            \n",
    "         # lowercase the text    \n",
    "        if text_lower_case:\n",
    "            doc = doc.lower()\n",
    "\n",
    "        # remove the common words\n",
    "        if removeCommonWords:\n",
    "            doc = remove_CommonWords(doc)\n",
    "            \n",
    "        # remove stopwords\n",
    "        if stopword_removal:\n",
    "            doc = remove_stopwords(doc, is_lower_case=text_lower_case, stopwords=stopwords)\n",
    "            \n",
    "        # remove extra whitespace\n",
    "        doc = re.sub(' +', ' ', doc)\n",
    "        doc = doc.strip()\n",
    "            \n",
    "        normalized_corpus.append(doc)\n",
    "        \n",
    "    return normalized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "id": "84bf63a9-bf04-44d6-a83f-b454402dba1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "corpus = normalize_corpus(dataFrame['raw text']) # does not contain punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "id": "4540da22-1928-4cc4-93ba-050e4a39c7fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "punct = dataFrame.copy()\n",
    "punct['raw text'] = normalize_corpus(punct['raw text']) # contains punctuations which is used to summarize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1473a3e3",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Add the cleaned text to the structure you created.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "id": "a0dc9b31-dd0c-4e22-b9d0-cf84b644048f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataFrame['clean text'] = corpus # Add the cleaned data to the clean text column of the dataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82cc947b",
   "metadata": {},
   "source": [
    "### Clean Up: Discussion\n",
    "Answer the questions below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1ba98d",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Which Smart City applicants did you remove? What issues did you see with the documents?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffebf5a5",
   "metadata": {},
   "source": [
    "The applicants I removed are Moreno Valley, Tallahassee, Reno, Toledo, and Lubbock. PdfReader was not able to read in text from the pdfs of these cities. The rows corresponding to these cities in the dataframe were all black and so there is no use of empty rows which led to the removal of these cities. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1620ed74",
   "metadata": {},
   "source": [
    "#### Explain what additional text processing methods you used and why."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae42fc81",
   "metadata": {},
   "source": [
    "I used 3 functions remove_States(), remove_CommonWords(), remove_Cites() other than the ones provided in the text_normalizer.py to remove words that may effect clustering and topic modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15817355",
   "metadata": {},
   "source": [
    "#### Did you identify any potientally problematic words?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ad6082",
   "metadata": {},
   "source": [
    "I found some problematic words looking at some of the cleaned data which are as follows:\n",
    "cteure\n",
    "nthern\n",
    "knik\n",
    "chlenge\n",
    "automo\n",
    "ci\n",
    "es\n",
    "There are many such words that don't mean anything."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1507fbe",
   "metadata": {},
   "source": [
    "## Experimenting with Clustering Models\n",
    "\n",
    "Now, you'll start to explore models to find the optimal clustering model. In this section, you'll explore [K-means](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html), [Hierarchical](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html), and [DBSCAN](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html#sklearn.cluster.DBSCAN) clustering algorithms.\n",
    "Create these algorithms with k_clusters for K-means and Hierarchical.\n",
    "For each cell in the table provide the [Silhouette score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html#sklearn.metrics.silhouette_score), [Calinski and Harabasz score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.calinski_harabasz_score.html#sklearn.metrics.calinski_harabasz_score), and [Davies-Bouldin score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.davies_bouldin_score.html#sklearn.metrics.davies_bouldin_score).\n",
    "\n",
    "In each cell, create an array to store the values.\n",
    "For example, \n",
    "\n",
    "|Algorithm| k = 9 | k = 18| k = 36 | Optimal k| \n",
    "|--|--|--|--|--|\n",
    "|K-means| [-0.01, 1.05, 0.95]| [-0.001, 1.07, 0.93] | [0.01, 1.15, 0.88] | [0.02, 1.14, 2.86] |\n",
    "|Hierarchical |[0.02, 1.37, 2.84]| [0.01, 1.30, 1.95]| [0.001, 1.33, 1.30] | [0.02, 1.57, 4.77]|\n",
    "|DBSCAN | X | X | X | [-0.15, 0.76, 2.52] |\n",
    "\n",
    "\n",
    "\n",
    "### Optimality \n",
    "You will need to find the optimal k for K-means and Hierarchical algorithms.\n",
    "Find the optimality for k in the range 2 to 50.\n",
    "Provide the code used to generate the optimal k and provide justification for your approach.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da2b033",
   "metadata": {},
   "source": [
    "|Algorithm| k = 9 | k = 18| k = 36 | Optimal k| \n",
    "|--|--|--|--|--|\n",
    "|K-means|--|--|--|--|\n",
    "|Hierarchical |--|--|--|--|\n",
    "|DBSCAN | X | X | X | -- |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "id": "2dc8c7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "import yellowbrick\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b45c5a-2008-4cb1-b7bb-e62f7320b659",
   "metadata": {
    "tags": []
   },
   "source": [
    "### K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "0d96d23a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Silhouette Score for k=9 is: -0.005641820615144266\n",
      "The Calinski-Harabasz Score for k=9 is: 1.0534943839056539\n",
      "The Davies-Bouldin Score for k=9 is: 0.9472888302537608\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(ngram_range=(1,3))\n",
    "tfidfdocs = tfidf.fit_transform(raw_documents=dataFrame['clean text'])\n",
    "\n",
    "#Instantiate kmeans\n",
    "kmeans = KMeans(n_clusters=9, random_state=42, n_init = 'auto')\n",
    "kmeans.fit_transform(tfidfdocs)\n",
    "\n",
    "# get the 3 scores\n",
    "s_score = silhouette_score(tfidfdocs, kmeans.labels_)\n",
    "ch_score = calinski_harabasz_score(tfidfdocs.toarray(), kmeans.labels_)\n",
    "db_score = davies_bouldin_score(tfidfdocs.toarray(), kmeans.labels_)\n",
    "\n",
    "print(f\"The Silhouette Score for k=9 is: {s_score}\")\n",
    "print(f\"The Calinski-Harabasz Score for k=9 is: {ch_score}\")\n",
    "print(f\"The Davies-Bouldin Score for k=9 is: {db_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "1cbec6f4-3d44-4f36-8aca-d1096df012a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Silhouette Score for k=18 is: -0.0019548933808776945\n",
      "The Calinski-Harabasz Score for k=18 is: 1.0748302337549769\n",
      "The Davies-Bouldin Score for k=18 is: 0.9300400195911703\n"
     ]
    }
   ],
   "source": [
    "#Instantiate kmeans\n",
    "kmeans = KMeans(n_clusters=18, random_state=42, n_init = 'auto')\n",
    "kmeans.fit_transform(tfidfdocs)\n",
    "\n",
    "# get the 3 scores\n",
    "s_score = silhouette_score(tfidfdocs, kmeans.labels_)\n",
    "ch_score = calinski_harabasz_score(tfidfdocs.toarray(), kmeans.labels_)\n",
    "db_score = davies_bouldin_score(tfidfdocs.toarray(), kmeans.labels_)\n",
    "\n",
    "print(f\"The Silhouette Score for k=18 is: {s_score}\")\n",
    "print(f\"The Calinski-Harabasz Score for k=18 is: {ch_score}\")\n",
    "print(f\"The Davies-Bouldin Score for k=18 is: {db_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "36c4139f-4e72-4ab2-b764-70aab8781772",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Silhouette Score for k=36 is: 0.008005542871905894\n",
      "The Silhouette Score for k=36 is: 1.1478962176625513\n",
      "The Davies-Bouldin Score for k=36 is: 0.8822652983606347\n"
     ]
    }
   ],
   "source": [
    "#Instantiate kmeans\n",
    "kmeans = KMeans(n_clusters=36, random_state=42, n_init = 'auto')\n",
    "kmeans.fit_transform(tfidfdocs)\n",
    "\n",
    "# get the 3 scores\n",
    "s_score = silhouette_score(tfidfdocs, kmeans.labels_)\n",
    "ch_score = calinski_harabasz_score(tfidfdocs.toarray(), kmeans.labels_)\n",
    "db_score = davies_bouldin_score(tfidfdocs.toarray(), kmeans.labels_)\n",
    "\n",
    "print(f\"The Silhouette Score for k=36 is: {s_score}\")\n",
    "print(f\"The Silhouette Score for k=36 is: {ch_score}\")\n",
    "print(f\"The Davies-Bouldin Score for k=36 is: {db_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975e32e1-b89f-4b3d-9aa2-b268b79a9945",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Hierarchical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "42480093-166f-4135-8583-19c7a14a7bcd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Silhouette Score for k=9 is: 0.01658593116603287\n",
      "The Calinski-Harabasz Score for k=9 is: 1.3678130099780141\n",
      "The Davies-Bouldin Score for k=9 is: 2.8425813956613966\n"
     ]
    }
   ],
   "source": [
    "#Instantiate agglomerative clustering which will be used for hierarchial clustering\n",
    "agg_clustering = AgglomerativeClustering(n_clusters=9)\n",
    "agg_clustering.fit(tfidfdocs.toarray())\n",
    "\n",
    "# get the 3 scores\n",
    "s_score = silhouette_score(tfidfdocs, agg_clustering.labels_)\n",
    "ch_score = calinski_harabasz_score(tfidfdocs.toarray(), agg_clustering.labels_)\n",
    "db_score = davies_bouldin_score(tfidfdocs.toarray(), agg_clustering.labels_)\n",
    "\n",
    "print(f\"The Silhouette Score for k=9 is: {s_score}\")\n",
    "print(f\"The Calinski-Harabasz Score for k=9 is: {ch_score}\")\n",
    "print(f\"The Davies-Bouldin Score for k=9 is: {db_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "894ef92d-21d7-442d-92b3-2f835fdf9a0c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Silhouette Score for k=18 is: 0.01299073283770277\n",
      "The Calinski-Harabasz Score for k=18 is: 1.301021242854046\n",
      "The Davies-Bouldin Score for k=18 is: 1.948378369245612\n"
     ]
    }
   ],
   "source": [
    "#Instantiate agglomerative clustering which will be used for hierarchial clustering\n",
    "agg_clustering = AgglomerativeClustering(n_clusters=18)\n",
    "agg_clustering.fit(tfidfdocs.toarray())\n",
    "\n",
    "# get the 3 scores\n",
    "s_score = silhouette_score(tfidfdocs, agg_clustering.labels_)\n",
    "ch_score = calinski_harabasz_score(tfidfdocs.toarray(), agg_clustering.labels_)\n",
    "db_score = davies_bouldin_score(tfidfdocs.toarray(), agg_clustering.labels_)\n",
    "\n",
    "print(f\"The Silhouette Score for k=18 is: {s_score}\")\n",
    "print(f\"The Calinski-Harabasz Score for k=18 is: {ch_score}\")\n",
    "print(f\"The Davies-Bouldin Score for k=18 is: {db_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "65a9535c-df59-418e-bef7-8965fe068889",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Silhouette Score for k=36 is: 0.001895066278233009\n",
      "The Calinski-Harabasz Score for k=36 is: 1.3250064929458445\n",
      "The Davies-Bouldin Score for k=36 is: 1.2975917086179654\n"
     ]
    }
   ],
   "source": [
    "#Instantiate agglomerative clustering which will be used for hierarchial clustering\n",
    "agg_clustering = AgglomerativeClustering(n_clusters=36)\n",
    "agg_clustering.fit(tfidfdocs.toarray())\n",
    "\n",
    "# get the 3 scores\n",
    "s_score = silhouette_score(tfidfdocs, agg_clustering.labels_)\n",
    "ch_score = calinski_harabasz_score(tfidfdocs.toarray(), agg_clustering.labels_)\n",
    "db_score = davies_bouldin_score(tfidfdocs.toarray(), agg_clustering.labels_)\n",
    "\n",
    "print(f\"The Silhouette Score for k=36 is: {s_score}\")\n",
    "print(f\"The Calinski-Harabasz Score for k=36 is: {ch_score}\")\n",
    "print(f\"The Davies-Bouldin Score for k=36 is: {db_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82514e5f-04b3-49e4-8d3e-60c76dda2536",
   "metadata": {
    "tags": []
   },
   "source": [
    "### DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "f86a100f-f845-4133-9ecf-04ff0eecf907",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Silhouette Score for DBSCAN is: -0.1546694561888406\n",
      "The Calinski-Harabasz Score for DBSCAN is: 0.762798217484352\n",
      "The Davies-Bouldin Score for DBSCAN is: 2.521451998337413\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "\n",
    "# Perform PCA to reduce the dimensionality of the dataset\n",
    "pca = PCA(n_components=69)\n",
    "tfidf_pca = pca.fit_transform(tfidfdocs.toarray())\n",
    "\n",
    "# Apply DBSCAN to the reduced dataset\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
    "dbscan.fit(tfidf_pca)\n",
    "\n",
    "# Calculate the evaluation metrics\n",
    "s_score = silhouette_score(tfidf_pca, dbscan.labels_)\n",
    "ch_score = calinski_harabasz_score(tfidf_pca, dbscan.labels_)\n",
    "db_score = davies_bouldin_score(tfidf_pca, dbscan.labels_)\n",
    "\n",
    "print(f\"The Silhouette Score for DBSCAN is: {s_score}\")\n",
    "print(f\"The Calinski-Harabasz Score for DBSCAN is: {ch_score}\")\n",
    "print(f\"The Davies-Bouldin Score for DBSCAN is: {db_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca04e7ba-b43d-42ec-8f2b-93940dc1f7f3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Optimal K for KMeans and Hierarchical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "id": "b4d8c04d-fc55-4108-af44-da81572869f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "def find_optimal_k(dataset, k_range):\n",
    "    # Perform PCA to reduce the dimensionality of the dataset\n",
    "    pca = PCA(n_components=min(dataset.shape)-1)\n",
    "    dataset_pca = pca.fit_transform(dataset)\n",
    "    \n",
    "    # Initialize lists to store evaluation scores for K-means and hierarchical clustering\n",
    "    kmeans_scores = []\n",
    "    hier_scores = []\n",
    "    \n",
    "    # Iterate over k values and perform clustering for both K-means and hierarchical clustering\n",
    "    for k in k_range:\n",
    "        # K-means clustering\n",
    "        kmeans = KMeans(n_clusters=k, n_init = 'auto')\n",
    "        kmeans.fit(dataset_pca)\n",
    "        kmeans_silhouette = silhouette_score(dataset_pca, kmeans.labels_)\n",
    "        kmeans_ch = calinski_harabasz_score(dataset_pca, kmeans.labels_)\n",
    "        kmeans_db = davies_bouldin_score(dataset_pca, kmeans.labels_)\n",
    "        kmeans_scores.append([k, kmeans_silhouette, kmeans_ch, kmeans_db])\n",
    "\n",
    "        # Hierarchical clustering\n",
    "        hier = AgglomerativeClustering(n_clusters=k)\n",
    "        hier.fit(dataset_pca)\n",
    "        hier_silhouette = silhouette_score(dataset_pca, hier.labels_)\n",
    "        hier_ch = calinski_harabasz_score(dataset_pca, hier.labels_)\n",
    "        hier_db = davies_bouldin_score(dataset_pca, hier.labels_)\n",
    "        hier_scores.append([k, hier_silhouette, hier_ch, hier_db])\n",
    "\n",
    "    # Convert scores to numpy arrays for easier manipulation\n",
    "    kmeans_scores = np.array(kmeans_scores)\n",
    "    hier_scores = np.array(hier_scores)\n",
    "\n",
    "    # Find optimal k value for K-means clustering based on highest Silhouette score\n",
    "    kmeans_optimal = kmeans_scores[np.argmax(kmeans_scores[:, 1]), 0]\n",
    "\n",
    "    # Find optimal k value for hierarchical clustering based on highest Silhouette score\n",
    "    hier_optimal = hier_scores[np.argmax(hier_scores[:, 1]), 0]\n",
    "\n",
    "    # Return optimal k values along with evaluation scores for both K-means and hierarchical clustering\n",
    "    kmeans_optimal_scores = kmeans_scores[np.argmax(kmeans_scores[:, 1])]\n",
    "    hier_optimal_scores = hier_scores[np.argmax(hier_scores[:, 1])]\n",
    "    return kmeans_optimal, hier_optimal, kmeans_optimal_scores, hier_optimal_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "id": "1877f816-6652-456c-ba10-48763b54f26a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-means optimal k: 2.0\n",
      "Hierarchical optimal k: 3.0\n",
      "K-means scores for optimal k:\n",
      " [2.         0.02236657 1.14706364 2.85851411]\n",
      "Hierarchical scores for optimal k:\n",
      " [3.         0.01872715 1.56890359 4.76816476]\n"
     ]
    }
   ],
   "source": [
    "#Find the optimal k value and its corresponding 3 scores\n",
    "k_range = range(2, 50)\n",
    "kmeans_optimal, hier_optimal, kmeans_scores, hier_scores = find_optimal_k(tfidfdocs.toarray(), k_range)\n",
    "\n",
    "print(\"K-means optimal k:\", kmeans_optimal)\n",
    "print(\"Hierarchical optimal k:\", hier_optimal)\n",
    "print(\"K-means scores for optimal k:\\n\", kmeans_scores)\n",
    "print(\"Hierarchical scores for optimal k:\\n\", hier_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c20fd69",
   "metadata": {},
   "source": [
    "#### How did you approach finding the optimal k?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753c54da",
   "metadata": {},
   "source": [
    "The function find_optimal_k uses the Silhouette score, Calinski and Harabasz score, and Davies-Bouldin score as evaluation metrics for K-means and hierarchical clustering for a range of k values. It then returns the k value that results in the highest Silhouette score, along with the corresponding evaluation scores for both K-means and hierarchical clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79ec635",
   "metadata": {},
   "source": [
    "#### What algorithm do you believe is the best? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3136b0a4",
   "metadata": {},
   "source": [
    "I would choose hierarchical clustering. It has the highest Calinski and Harabasz score, which indicates good separation between clusters, and a relatively high Silhouette score, which indicates the compactness of clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e45a2a3",
   "metadata": {},
   "source": [
    "### Add Cluster ID to output file\n",
    "In your data structure, add the cluster id for each smart city respectively. Show the to append the clusterid code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "id": "9ad83e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimal k for K-means clustering\n",
    "optimal_k = 2\n",
    "\n",
    "# Fit KMeans model with optimal k\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init='auto')\n",
    "kmeans.fit(tfidfdocs)\n",
    "\n",
    "# Assign cluster labels to each data point\n",
    "cluster_labels = kmeans.labels_\n",
    "\n",
    "# Add cluster id to DataFrame\n",
    "dataFrame['clusterid'] = cluster_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959e7275",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Save Model\n",
    "\n",
    "After finding the best model, it is desirable to have a way to persist the model for future use without having to retrain. Save the model using [model persistance](https://scikit-learn.org/stable/model_persistence.html). This model should be saved in the same directory as this notebook and should be loaded as the model for your `project3.py`.\n",
    "\n",
    "Save the model as `model.pkl`. You do not have to use pickle, but be sure to save the persistance using one of the methods listed in the link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "id": "1000077c-bfeb-429b-b9b9-b1429923c2b0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vectorizer.pkl']"
      ]
     },
     "execution_count": 538,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump, load\n",
    "\n",
    "# Select the input variables as X\n",
    "X = dataFrame['clean text']\n",
    "\n",
    "# Select the target variable as Y\n",
    "Y = dataFrame['clusterid']\n",
    "\n",
    "# Create a TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf = vectorizer.fit_transform(X)\n",
    "\n",
    "# Train the KMeans model with optimal k\n",
    "kmeans = KMeans(n_clusters=2, random_state=42, n_init='auto')\n",
    "kmeans.fit(tfidf)\n",
    "\n",
    "# Save the model and the vectorizer, the vectorizer will also be used for prediction\n",
    "dump(kmeans, 'model.pkl')\n",
    "dump(vectorizer, 'vectorizer.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe5a0c9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Derving Themes and Concepts\n",
    "\n",
    "Perform Topic Modeling on the cleaned data. Provide the top five words for `TOPIC_NUM = Best_k` as defined in the section above. Feel free to reference [Chapter 6](https://github.com/dipanjanS/text-analytics-with-python/tree/master/New-Second-Edition/Ch06%20-%20Text%20Summarization%20and%20Topic%20Models) for more information on Topic Modeling and Summarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "id": "b684bc24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T1: vehicle transit project include technology\n",
      "T2: vehicle transit public project technology\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim import corpora, models\n",
    "\n",
    "# Define the number of topics\n",
    "TOPIC_NUM = 2\n",
    "\n",
    "# Convert the cleaned data to a list of tokens\n",
    "docs = [doc.split() for doc in dataFrame['clean text']]\n",
    "\n",
    "# Create a dictionary of the tokens\n",
    "dictionary = corpora.Dictionary(docs)\n",
    "\n",
    "# Create a document-term matrix\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in docs]\n",
    "\n",
    "# Train the LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(doc_term_matrix, num_topics=TOPIC_NUM, id2word = dictionary, passes=75)\n",
    "\n",
    "# Show the top 5 words for each topic\n",
    "topics = lda_model.show_topics(num_topics=TOPIC_NUM, num_words=5, formatted=False)\n",
    "for i, topic in enumerate(topics):\n",
    "    print(f\"T{i+1}: {' '.join([word[0] for word in topic[1]])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915516dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2a3896",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1573fe65",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Extract themes\n",
    "Write a theme for each topic (atleast a sentence each)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9737d8df",
   "metadata": {},
   "source": [
    "T1 represents the themes related to the use of advanced technology and innovative transportation systems for developing smart cities, with a focus on efficient and sustainable vehicle and transit projects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21cf24c6",
   "metadata": {},
   "source": [
    "T2 represents the themes related to the inclusion of public and community-driven projects in smart city development, with a focus on promoting the use of technology and innovative transit systems to improve the quality of life for citizens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ab7df0",
   "metadata": {},
   "source": [
    "### Add Topid ID to output file\n",
    "Add the top two topics for each smart city to the data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "id": "3e937841",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFrame['topicids'] = dataFrame['clean text'].apply(lambda x: \n",
    "    (\"T1, T2\") if all(word in x for word in ['vehicle', 'transit', 'project', 'technology', 'include', 'vehicle', 'transit', 'project', 'technology', 'public']) \n",
    "    else (\"T1\" if all(word in x for word in ['vehicle', 'transit', 'project', 'include', 'technology']) \n",
    "    else (\"T2\" if all(word in x for word in ['vehicle', 'transit', 'public', 'project', 'technology'])\n",
    "    else \"\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f9c240",
   "metadata": {},
   "source": [
    "## Gathering Applicant Summaries and Keywords\n",
    "\n",
    "For each smart city applicant, gather a summary and keywords that are important to that document. You can use gensim to do this. Here are examples of functions that you could use.\n",
    "\n",
    "```python\n",
    "\n",
    "from gensim.summarization import summarize\n",
    "\n",
    "def summary(text, ratio=0.2, word_count=250, split=False):\n",
    "    return summarize(text, ratio= ratio, word_count=word_count, split=split)\n",
    "    \n",
    "from gensim.summarization import keywords\n",
    "\n",
    "def keys(text, ratio=0.01):\n",
    "    return keywords(text, ratio=ratio)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "id": "21cc2dda-30b5-404d-9d1d-75c6d7dda866",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from heapq import nlargest\n",
    "import pandas as pd\n",
    "from string import punctuation\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "# Define the functions for summarization and keyword extraction\n",
    "def summary(text, ratio=0.2, word_count=50):\n",
    "    sentences = sent_tokenize(text)\n",
    "    words = word_tokenize(text)\n",
    "    words = [word for word in words if word.isalnum() and word not in stopwords.words('english')]\n",
    "    word_freq = FreqDist(words)\n",
    "    sentence_scores = []\n",
    "    for sentence in sentences:\n",
    "        sentence_words = word_tokenize(sentence)\n",
    "        sentence_words = [word for word in sentence_words if word.isalnum()]\n",
    "        sentence_score = sum([word_freq[word] for word in sentence_words])\n",
    "        sentence_scores.append((sentence_score, sentence))\n",
    "    sentence_scores = sorted(sentence_scores, reverse=True)\n",
    "    selected_sentences = [sentence for score, sentence in sentence_scores][:int(len(sentences)*ratio)]\n",
    "    summary = \" \".join(selected_sentences)\n",
    "    return summary[:word_count]\n",
    "\n",
    "def get_keywords(text):\n",
    "    # Preprocess the text\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in tokens if word.isalnum() and not word in stop_words]\n",
    "\n",
    "    # Calculate the frequency distribution of the words\n",
    "    freq_dist = FreqDist(words)\n",
    "\n",
    "    # Extract the top 5 keywords\n",
    "    keywords = [word for word, freq in freq_dist.most_common(5)]\n",
    "\n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d27ce37",
   "metadata": {},
   "source": [
    "### Add Summaries and Keywords\n",
    "Add summary and keywords to output file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "id": "c09357ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the summarization and keyword extraction functions to the DataFrame\n",
    "dataFrame['summary'] = punct['raw text'].apply(lambda x: summary(x, ratio=0.2))\n",
    "dataFrame['keywords'] = dataFrame['clean text'].apply(lambda x: get_keywords(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "id": "f0095867-1542-45d4-ba07-10e7614d2ad4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 city                                           raw text   \n",
      "0        AK Anchorage    CONTENTS \\n1 VISION ...........................  \\\n",
      "1       AL Birmingham  aBirmingham\\nRising\\nBirmingham Rising! Meetin...   \n",
      "2       AL Montgomery   \\n \\n U.S. Department of Transportation - “BE...   \n",
      "3    AZ Scottsdale AZ    \\n  \\n \\n \\n \\nFederal Agency Name:   U.S. D...   \n",
      "4           AZ Tucson  Tucson Smart City Demonstration Proposal\\nPart...   \n",
      "..                ...                                                ...   \n",
      "64        VA Richmond    \\n \\n \\n   \\n \\n \\n  \\n      Contact Informa...   \n",
      "65  VA Virginia Beach    \\n 1.  Project Vision  ........................   \n",
      "66         WA Seattle  Beyond Traffic: USDOT Smart City Challenge\\nAp...   \n",
      "67         WA Spokane  USDOT Smart City Challenge -  Spokane  \\nPage ...   \n",
      "68         WI Madison  Building a Smart Madison  \\nfor Shared Prosper...   \n",
      "\n",
      "                                           clean text  clusterid topicids   \n",
      "0   content vision population chaestics siteapproa...          1   T1, T2  \\\n",
      "1   rise rise meet challengebecome americas next i...          0   T1, T2   \n",
      "2   department beyond srt chlenge proposal ntgory ...          1   T1, T2   \n",
      "3   federal agency names department uot federal hi...          1   T1, T2   \n",
      "4   demonstration proposal part vision narrative f...          1   T1, T2   \n",
      "..                                                ...        ...      ...   \n",
      "64  contact information contact michael sawyer pho...          1   T1, T2   \n",
      "65  project vision project demographics project al...          1   T1, T2   \n",
      "66  beyond uot challenge application prepare depar...          1   T1, T2   \n",
      "67  uot challenge connected innovation platform pa...          1   T1, T2   \n",
      "68  build shared prosperity application beyond cha...          1   T1, T2   \n",
      "\n",
      "                                              summary   \n",
      "0   content 1 vision ................................  \\\n",
      "1   overall goalthis eff ort wouldto quanand evalu...   \n",
      "2   team partners processes team rers &amp; key st...   \n",
      "3   federal agency name : u.s. department ( uot ) ...   \n",
      "4   safety c. environmental safetychlengestrike cr...   \n",
      "..                                                ...   \n",
      "64  srt chlenge grant applicationorganization supp...   \n",
      "65  risk matrix category description risk probabil...   \n",
      "66  project follow establish goal : enhance r mobi...   \n",
      "67  university district projects project name type...   \n",
      "68  incident include weather warning , campus clos...   \n",
      "\n",
      "                                             keywords  \n",
      "0   [vision, include, public, information, archite...  \n",
      "1    [mobility, technology, transit, vehicle, center]  \n",
      "2        [transit, university, public, srt, planning]  \n",
      "3     [vehicle, program, plan, technology, collision]  \n",
      "4   [vehicle, demonstration, management, data, tra...  \n",
      "..                                                ...  \n",
      "64      [vehicle, transit, public, application, data]  \n",
      "65     [vehicle, project, service, technology, beach]  \n",
      "66             [data, project, private, public, time]  \n",
      "67    [challenge, university, uot, district, transit]  \n",
      "68          [platform, vehicle, shared, data, vision]  \n",
      "\n",
      "[69 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "print(dataFrame)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9cb2c4",
   "metadata": {},
   "source": [
    "## Write output data\n",
    "\n",
    "The output data should be written as a TSV file.\n",
    "You can use `to_csv` method from Pandas for this if you are using a DataFrame.\n",
    "\n",
    "`Syntax: df.to_csv('file.tsv', sep = '')` \\\n",
    "`df.to_csv('smartcity_eda.tsv', sep='\\t')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "id": "58827464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save DataFrame to output file\n",
    "dataFrame.to_csv('smartcity_eda.tsv', sep = '\\t', escapechar = '\\\\')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a18b8ff",
   "metadata": {},
   "source": [
    "# Moving Forward\n",
    "Now that you have explored the dataset, take the important features and functions to create your `project3.py`.\n",
    "Please refer to the project spec for more guidance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6675ba",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
